## OpenShift Infrastructure Nodes
OpenShiftのサブスクリプションモデルでは、顧客はさまざまなコアを実行することができます。
インフラストラクチャコンポーネントを追加料金なしで利用できます。つまり、ノード
コアのOpenShiftインフラストラクチャコンポーネントのみを実行している場合はカウントされません。
をカバーするのに必要な総加入者数に換算すると
の環境のことを指します。

インフラストラクチャの分類に該当するOpenShiftコンポーネント

* kubernetes and OpenShift control plane services ("masters")
* router
* container image registry
* cluster metrics collection ("monitoring")
* cluster aggregated logging
* service brokers

上記に記載されていないコンテナ/ポッド/コンポーネントを実行しているノードはすべて
ワーカーノードであり、サブスクリプションでカバーされている必要があります。

### 機械セットの詳細
`MachineSets`の演習では、`MachineSets`を使用して、スケーリングを検討しました。
のレプリカ数を変更することで、クラスタを変更することができます。インフラストラクチャの場合
ノードを使用して、特定の Kubernetes
のラベルを使用しています。そして、さまざまなインフラストラクチャコンポーネントを設定して、以下のように実行するようにします。
は、これらのラベルを持つノードに特化しています。

[NOTE]
====
現在、インフラストラクチャコンポーネントを制御するために使用される演算子は、以下のようなことを行います。
テインツやトレラントの使用をすべてが支持しているわけではありません。これは、以下のことを意味します。
インフラストラクチャのワークロードはインフラストラクチャノードに移行しますが、他の
ワークロードがインフラストラクチャ上で実行されることが特に妨げられているわけではありません。
ノードを使用しています。言い換えれば、ユーザーのワークロードはインフラストラクチャと混ざり合う可能性があります。
テイン/トレレーションのサポートがすべてのオペレータに完全に実装されるまでの間、作業負荷を軽減することができます。

テインツ/トレレーションの使用は、これらの演習ではカバーされていません。
====

これを実現するために、`MachineSets`を追加で作成します。

`MachineSets`がどのように動作するかを理解するためには、以下を実行してください。

これにより、以下の議論の一部をフォローすることができます。

[source,bash,role="copypaste copypaste-warning"]
----
oc get machineset -n openshift-machine-api -o yaml cluster-5fa6-hx2ml-worker-us-east-2c
----

#### Metadata
MachineSet`の`metadata`自体には、名前のような情報が含まれています。
の`MachineSet`と様々なラベルの

```YAML
metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400
```

[NOTE]
====
もし、`MachineSet`をダンプした場合、`MachineSet`に `annotations` が表示されるかもしれません。
のように、`MachineAutoScaler` が定義されているものを指定します。
====

#### Selector
`MachineSet` は `Machine` の作成方法を定義し、`Selector` は `MachineSet` を指定します。
どのマシンがセットに関連付けられているかをオペレータに知らせます。

```YAML
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

この場合、クラスタ名は `190125-3` であり、追加の
のラベルを指定します。

### テンプレートのメタデータ
テンプレートは、`MachineSet`の一部であり、それをテンプレート化するものである。
マシン`です。テンプレート`自体にメタデータを関連付けることができます。
変更を加える際には、ここで物事が一致していることを確認してください。

```YAML
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

#### テンプレート仕様
テンプレートは、`Machine`/Node`をどのように作成するかを指定する必要があります。
spec`、より具体的には `providerSpec` であることに気づくでしょう。
には、`Machine`を作成するための重要なAWSデータがすべて含まれています。
を正しく継承し、ブートストラップされます。

このケースでは、結果として得られるノードが1つ以上の
特定のラベルを使用します。上の例で見たように、ラベルは
メタデータ`セクションを参照してください。

```YAML
  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...
```

デフォルトでは、インストーラが作成する `MachineSets` には
ノードにラベルを追加します。

### カスタムマシンセットの定義
既存の`MachineSet`を解析したので、次は、`MachineSet`を解析してみましょう。
1つを作成するためのルール、少なくとも私たちが行っているような単純な変更のために。

1. providerSpec` の中では何も変更しない。
2. machine.openshift.io/cluster-api-clusterのインスタンスを変更しないでください。<clusterid>``のインスタンスを変更しないでください。
3. `MachineSet` にユニークな `name` を与えます。
4. `machine.openshift.io/cluster-api-machineset` のインスタンスが `name` と一致することを確認します。
5. ノードに必要なラベルを `.spec.template.spec.metadata.l labels` に追加します。
6. `MachineSet` `name` の参照を変更する場合でも、`subnet` を変更しないように注意してください。

これは複雑に聞こえるかもしれませんが、小さなプログラムといくつかのステップがあります。
があなたのためにハードワークをしてくれます。


[source,bash,role="execute"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=3
----

次のように実行します。

[source,bash,role="execute"]
----
oc get machineset -n openshift-machine-api
----

以下のような名前で記載された新しいインフラセットが表示されているはずです。

```
...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...
```

まだインスタンスが起動していてブートストラップを行っているため、セットの中には利用可能なマシンがありません。
インスタンスがいつ起動するかは `oc get machine -n openshift-machine-api` で確認することができます。
次に `oc get node` を使うと、実際のノードがいつ結合されて準備が整ったかを確認することができます。

[注意]
====
マシン」が準備されて「ノード」として追加されるまでには数分かかることがあります。
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2
```

どのノードが新しいノードなのか分からなくて困っている場合は、`AGE`カラムを見てみてください。
これが一番若いノードでしょう! また、`ROLES` 列では、新しいノードが `worker` と `infra` の両方のロールを持っていることに気づくでしょう。

### ラベルを確認する
この例では、一番若いノードは `ip-10-0-128-138.us-east-1.compute.internal` という名前でした。

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-139-81.us-east-2.compute.internal --show-labels
----

そして、`LABELS`の欄には、次のように書かれています。

    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

見えにくいですが、`node-role.kubernetes.io/infra`ラベルがあります。

### マシンセットを追加する(またはスケールする)
現実的な本番環境では、インフラストラクチャコンポーネントを保持するために、少なくとも3つの`MachineSets`が必要です。
ロギングアグリゲーションソリューションとサービスメッシュの両方がElasticSearchをデプロイするので、ElasticSearchは3つのノードに分散した3つのインスタンスを必要とします。
なぜ3つの `MachineSets` が必要なのか?
理論的には、異なるAZに複数の `MachineSets` を配置することで、AWSがAZを失っても真っ暗になることはありません。

スクリプトレットで作成した `MachineSet` はすでに3つのレプリカを作成しているので、今は何もする必要はありません。
また、自分で追加のレプリカを作成する必要もありません。

### 追加クレジット
openshift-machine-api` プロジェクトにはいくつかの `Pods` があります。
そのうちの一つは `machine-api-controllers-56bdc6874f-86jnb` のような名前である。その `Pod` のコンテナ上で `oc log` を使うと、ノードを実際に生成するためのさまざまな演算子のビットを見ることができます。

## クイック演算子の背景
オペレータはただの `Pods` です。しかし、それらは特別な `Pods` です。
それらは、Kubernetes環境でアプリケーションをデプロイして管理する方法を理解しているソフトウェアです。
Operatorsの力は、`CustomResourceDefinitions` (`CRD`)と呼ばれる最近のKubernetesの機能に依存しています。
CRD`はまさにその名の通りの機能です。これらはカスタムリソースを定義する方法であり、本質的にはKubernetes APIを新しいオブジェクトで拡張するものです。

Kubernetes で `Foo` オブジェクトを作成、読み込み、更新、削除できるようにしたい場合、`Foo` リソースとは何か、どのように動作するかを定義した `CRD` を作成します。
そして、`CRD`のインスタンスである `CustomResources` (`CRs`) を作成することができます。

オペレータの場合、一般的なパターンとしては、オペレータが `CRs` を見て設定を行い、Kubernetes 環境上で _operate_ を行い、設定で指定されたことを実行するというものです。
ここでは、OpenShiftのインフラストラクチャオペレータのいくつかがどのように動作するかを見てみましょう。

## インフラストラクチャコンポーネントの移動
これで特別なノードができたので、インフラストラクチャのコンポーネントをその上に移動させることができます。

### ルータ
OpenShiftルータは `openshift-ingress-operator` という `Operator` によって管理されています。
その `Pod` は `openshift-ingress-operator` プロジェクトに存在する。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルトのルータのインスタンスは `openshift-ingress` プロジェクトにあります。 Pods` を見てみましょう。

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

以下のように確認できます。

```
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   <none>
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   <none>
```

ルータが動作している `Node` を確認します。

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-144-70.us-east-2.compute.internal
----

それが `worker` の役割を持っていることがわかるだろう。

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

ルータオペレータのデフォルトの設定では、`worker`の役割を持つノードをピックするようになっています。
しかし、専用のインフラストラクチャノードを作成したので、ルータインスタンスを `infra` の役割を持つノードに配置するようにオペレータに指示したい。

OpenShiftのルーターオペレータは、`ingresses.config.openshift.io`というカスタムリソース定義(`CRD`)を使用して、クラスタのデフォルトルーティングサブドメインを定義します。

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトはマスタだけでなくルータオペレータにも観測されます。あなたのはこんな感じでしょう

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

個々のルータのデプロイは `ingresscontrollers.operator.openshift.io` CRD で管理されます。
名前空間 `openshift-ingress-operator` に作成されたデフォルトのものがあります。

[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

以下のようになります。

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
```

ルータポッドがインフラストラクチャノードにヒットするように指示する `nodeSelector` を指定するには、以下の設定を適用します。

[source,bash,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ingresscontroller.yaml
----


実行:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[NOTE]
====
ルーターの移動中にセッションがタイムアウトすることがあります。
ページを更新してセッションを取り戻してください。
端末セッションが失われることはありませんが、手動でこのページに戻る必要があるかもしれません。
====

もし十分に手際が良ければ、`Terminating` か `ContainerCreating` のいずれかのポッドを捕まえることができるかもしれません。
Terminating` ポッドはワーカーノードの1つで動作していました。
実行中の`Running` ポッドは最終的に `infra` ロールを持つノードの1つで動作しています。

## レジストリ
レジストリは、オペレータが実際のレジストリポッドをどのように展開するかを設定するために、同様の `CRD` メカニズムを使用します。
このCRDは `configs.imageregistry.operator.openshift.io` です。
このCRDに `nodeSelector` を追加するために `cluster` のCRDオブジェクトを編集します。まず、それを見てみましょう。

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

以下のように確認できます。

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...
```

次のコマンドを実行します。

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

上記コマンドによって、レジストリCRの `.spec` を修正し、`nodeSelector` を追加します。

[NOTE]
====
この時点では、画像レジストリは演算子のために別のプロジェクトを使用していません。
演算子とオペランドは両方とも `openshift-image-registry` プロジェクトの中にあります。
====

パッチコマンドを実行すると、レジストリポッドがinfraノードに移動しているのがわかるはずです。
レジストリは `openshift-image-registry` プロジェクトにあります。

以下を素早く実行してみてください。


[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

古いレジストリポッドが終了し、新しいレジストリポッドが起動しているのがわかるかもしれません。
レジストリはS3バケットによってバックアップされているので、新しいレジストリポッドのインスタンスがどのノードにあるかは問題ではありません。
これはAPI経由でオブジェクトストアと通信しているので、そこに保存されている既存のイメージはすべてアクセス可能なままです。

また、デフォルトのレプリカ数は1であることにも注意してください。
現実の環境では、可用性やネットワークのスループットなどの理由から、このレプリカ数を増やしたいと思うかもしれません。

レジストリが着地したノード(ルータのセクションを参照)を見てみると、それが現在infraワーカー上で実行されていることに気づくでしょう。

最後に、イメージレジストリの設定のための `CRD` が名前空間ではなく、クラスタスコープになっていることに注目してください。
OpenShiftクラスタごとに内部/統合レジストリは1つしかありません。

## モニタリング
クラスタ監視オペレータは、Prometheus+Grafana+AlertManagerクラスタ監視スタックの展開と状態管理を担当します。
これは、クラスタの初期インストール時にデフォルトでインストールされます。
このオペレータは `openshift-monitoring` プロジェクトの `ConfigMap` を利用して、監視スタックの動作のために様々なチューニングや設定を行います。

以下の `ConfigMap` 定義は、インフラストラクチャノードにデプロイされる監視ソリューションを設定するものである。


```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
```

インストールの一部として作成された `ConfigMap` は存在しません。これがない場合、オペレータはデフォルトの設定を仮定します。
クラスタに `ConfigMap` が定義されていないことを確認してください。

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

以下のように出力されるはずです。

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```

Operatorは、様々なモニタリングスタックコンポーネントのためにいくつかの `ConfigMap` オブジェクトを作成します。

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

次のコマンドで新しいモニタリング設定を作成できます。

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/cluster-monitoring-configmap.yaml
----

モニタリングポッドが `worker` から `infra` `Nodes` に移動するのを見てみましょう。

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

または

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----

## ロギング
OpenShiftのログ集約ソリューションはデフォルトではインストールされていません。
ロギングの設定とデプロイメントを行う専用のラボ演習があります。
