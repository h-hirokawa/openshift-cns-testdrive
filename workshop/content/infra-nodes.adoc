## OpenShift Infrastructure Nodes
OpenShiftのサブスクリプションモデルでは、顧客はさまざまなコアを実行することができます。
インフラストラクチャコンポーネントを追加料金なしで利用できます。つまり、ノード
コアのOpenShiftインフラストラクチャコンポーネントのみを実行している場合はカウントされません。
をカバーするのに必要な総加入者数に換算すると
の環境のことを指します。

インフラストラクチャの分類に該当するOpenShiftコンポーネント

* kubernetes and OpenShift control plane services ("masters")
* router
* container image registry
* cluster metrics collection ("monitoring")
* cluster aggregated logging
* service brokers

上記に記載されていないコンテナ/ポッド/コンポーネントを実行しているノードはすべて
ワーカーノードであり、サブスクリプションでカバーされている必要があります。

### 機械セットの詳細
`MachineSets`の演習では、`MachineSets`を使用して、スケーリングを検討しました。
のレプリカ数を変更することで、クラスタを変更することができます。インフラストラクチャの場合
ノードを使用して、特定の Kubernetes
のラベルを使用しています。そして、さまざまなインフラストラクチャコンポーネントを設定して、以下のように実行するようにします。
は、これらのラベルを持つノードに特化しています。

[NOTE]
====
現在、インフラストラクチャコンポーネントを制御するために使用される演算子は、以下のようなことを行います。
テインツやトレラントの使用をすべてが支持しているわけではありません。これは、以下のことを意味します。
インフラストラクチャのワークロードはインフラストラクチャノードに移行しますが、他の
ワークロードがインフラストラクチャ上で実行されることが特に妨げられているわけではありません。
ノードを使用しています。言い換えれば、ユーザーのワークロードはインフラストラクチャと混ざり合う可能性があります。
テイン/トレレーションのサポートがすべてのオペレータに完全に実装されるまでの間、作業負荷を軽減することができます。

テインツ/トレレーションの使用は、これらの演習ではカバーされていません。
====

これを実現するために、`MachineSets`を追加で作成します。

`MachineSets`がどのように動作するかを理解するためには、以下を実行してください。

これにより、以下の議論の一部をフォローすることができます。

[source,bash,role="copypaste copypaste-warning"]
----
oc get machineset -n openshift-machine-api -o yaml cluster-5fa6-hx2ml-worker-us-east-2c
----

#### Metadata
MachineSet`の`metadata`自体には、名前のような情報が含まれています。
の`MachineSet`と様々なラベルの

```YAML
metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400
```

[NOTE]
====
もし、`MachineSet`をダンプした場合、`MachineSet`に `annotations` が表示されるかもしれません。
のように、`MachineAutoScaler` が定義されているものを指定します。
====

#### Selector
`MachineSet` は `Machine` の作成方法を定義し、`Selector` は `MachineSet` を指定します。
どのマシンがセットに関連付けられているかをオペレータに知らせます。

```YAML
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

この場合、クラスタ名は `190125-3` であり、追加の
のラベルを指定します。

### テンプレートのメタデータ
テンプレートは、`MachineSet`の一部であり、それをテンプレート化するものである。
マシン`です。テンプレート`自体にメタデータを関連付けることができます。
変更を加える際には、ここで物事が一致していることを確認してください。

```YAML
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

#### テンプレート仕様
テンプレートは、`Machine`/Node`をどのように作成するかを指定する必要があります。
spec`、より具体的には `providerSpec` であることに気づくでしょう。
には、`Machine`を作成するための重要なAWSデータがすべて含まれています。
を正しく継承し、ブートストラップされます。

このケースでは、結果として得られるノードが1つ以上の
特定のラベルを使用します。上の例で見たように、ラベルは
メタデータ`セクションを参照してください。

```YAML
  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...
```

デフォルトでは、インストーラが作成する `MachineSets` には
ノードにラベルを追加します。

### カスタムマシンセットの定義
既存の`MachineSet`を解析したので、次は、`MachineSet`を解析してみましょう。
1つを作成するためのルール、少なくとも私たちが行っているような単純な変更のために。

1. providerSpec` の中では何も変更しない。
2. machine.openshift.io/cluster-api-clusterのインスタンスを変更しないでください。<clusterid>``のインスタンスを変更しないでください。
3. `MachineSet` にユニークな `name` を与えます。
4. `machine.openshift.io/cluster-api-machineset` のインスタンスが `name` と一致することを確認します。
5. ノードに必要なラベルを `.spec.template.spec.metadata.l labels` に追加します。
6. `MachineSet` `name` の参照を変更する場合でも、`subnet` を変更しないように注意してください。

これは複雑に聞こえるかもしれませんが、小さなプログラムといくつかのステップがあります。
があなたのためにハードワークをしてくれます。


[source,bash,role="execute"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=3
----

次のように実行します。

[source,bash,role="execute"]
----
oc get machineset -n openshift-machine-api
----

以下のような名前で記載された新しいインフラセットが表示されているはずです。

```
...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...
```

まだインスタンスが起動していてブートストラップを行っているため、セットの中には利用可能なマシンがありません。
インスタンスがいつ起動するかは `oc get machine -n openshift-machine-api` で確認することができます。
次に `oc get node` を使うと、実際のノードがいつ結合されて準備が整ったかを確認することができます。

[注意]
====
マシン」が準備されて「ノード」として追加されるまでには数分かかることがあります。
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2
```

どのノードが新しいノードなのか分からなくて困っている場合は、`AGE`カラムを見てみてください。
これが一番若いノードでしょう! また、`ROLES` 列では、新しいノードが `worker` と `infra` の両方のロールを持っていることに気づくでしょう。

### ラベルを確認する
この例では、一番若いノードは `ip-10-0-128-138.us-east-1.compute.internal` という名前でした。

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-139-81.us-east-2.compute.internal --show-labels
----

そして、`LABELS`の欄には、次のように書かれています。

    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

見えにくいですが、`node-role.kubernetes.io/infra`ラベルがあります。

### マシンセットを追加する(またはスケールする)
現実的な本番環境では、インフラストラクチャコンポーネントを保持するために、少なくとも3つの`MachineSets`が必要です。
ロギングアグリゲーションソリューションとサービスメッシュの両方がElasticSearchをデプロイするので、ElasticSearchは3つのノードに分散した3つのインスタンスを必要とします。
なぜ3つの `MachineSets` が必要なのか?
理論的には、異なるAZに複数の `MachineSets` を配置することで、AWSがAZを失っても真っ暗になることはありません。

スクリプトレットで作成した `MachineSet` はすでに3つのレプリカを作成しているので、今は何もする必要はありません。
また、自分で追加のレプリカを作成する必要もありません。

### 追加クレジット
openshift-machine-api` プロジェクトにはいくつかの `Pods` があります。
そのうちの一つは `machine-api-controllers-56bdc6874f-86jnb` のような名前である。その `Pod` のコンテナ上で `oc log` を使うと、ノードを実際に生成するためのさまざまな演算子のビットを見ることができます。

## クイック演算子の背景
オペレータはただの `Pods` です。しかし、それらは特別な `Pods` です。
それらは、Kubernetes環境でアプリケーションをデプロイして管理する方法を理解しているソフトウェアです。
Operatorsの力は、`CustomResourceDefinitions` (`CRD`)と呼ばれる最近のKubernetesの機能に依存しています。
CRD`はまさにその名の通りの機能です。これらはカスタムリソースを定義する方法であり、本質的にはKubernetes APIを新しいオブジェクトで拡張するものです。

Kubernetes で `Foo` オブジェクトを作成、読み込み、更新、削除できるようにしたい場合、`Foo` リソースとは何か、どのように動作するかを定義した `CRD` を作成します。
そして、`CRD`のインスタンスである `CustomResources` (`CRs`) を作成することができます。

オペレータの場合、一般的なパターンとしては、オペレータが `CRs` を見て設定を行い、Kubernetes 環境上で _operate_ を行い、設定で指定されたことを実行するというものです。
ここでは、OpenShiftのインフラストラクチャオペレータのいくつかがどのように動作するかを見てみましょう。

## インフラストラクチャコンポーネントの移動
これで特別なノードができたので、インフラストラクチャのコンポーネントをその上に移動させることができます。

### ルータ
OpenShiftルータは `openshift-ingress-operator` という `Operator` によって管理されています。
その `Pod` は `openshift-ingress-operator` プロジェクトに存在する。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルトのルータのインスタンスは `openshift-ingress` プロジェクトにあります。 Pods` を見てみましょう。

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

以下のように確認できます。

```
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   <none>
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   <none>
```

ルータが動作している `Node` を確認します。

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-144-70.us-east-2.compute.internal
----

それが `worker` の役割を持っていることがわかるだろう。

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

ルータオペレータのデフォルトの設定では、`worker`の役割を持つノードをピックするようになっています。
しかし、専用のインフラストラクチャノードを作成したので、ルータインスタンスを `infra` の役割を持つノードに配置するようにオペレータに指示したい。

OpenShiftのルーターオペレータは、`ingresses.config.openshift.io`というカスタムリソース定義(`CRD`)を使用して、クラスタのデフォルトルーティングサブドメインを定義します。

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトはマスタだけでなくルータオペレータにも観測されます。あなたのはこんな感じでしょう

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

個々のルータのデプロイは `ingresscontrollers.operator.openshift.io` CRD で管理されます。
名前空間 `openshift-ingress-operator` に作成されたデフォルトのものがあります。

[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

以下のようになります。

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
```

ルータポッドがインフラストラクチャノードにヒットするように指示する `nodeSelector` を指定するには、以下の設定を適用します。

[source,bash,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ingresscontroller.yaml
----


Run:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[NOTE]
====
Your session may timeout during the router move. Please refresh the page to
get your session back. You will not lose your terminal session but may have
to navigate back to this page manually.
====

If you're quick enough, you might catch either `Terminating` or
`ContainerCreating` pods. The `Terminating` pod was running on one of the
worker nodes. The `Running` pods eventually are on one of our nodes with the
`infra` role.

## Registry
The registry uses a similar `CRD` mechanism to configure how the operator
deploys the actual registry pods. That CRD is
`configs.imageregistry.operator.openshift.io`. You will edit the `cluster` CR
object in order to add the `nodeSelector`. First, take a look at it:

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

You will see something like:

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...
```

If you run the following command:

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

It will modify the `.spec` of the registry CR in order to add the desired `nodeSelector`.

[NOTE]
====
At this time the image registry is not using a separate project for its
operator. Both the operator and the operand are housed in the
`openshift-image-registry` project.
====

After you run the patch command you should see the registry pod being moved to the
infra node. The registry is in the `openshift-image-registry` project. If you
execute the following quickly enough:

[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

You might see the old registry pod terminating and the new one starting.
Since the registry is being backed by an S3 bucket, it doesn't matter what
node the new registry pod instance lands on. It's talking to an object store
via an API, so any existing images stored there will remain accessible.

Also note that the default replica count is 1. In a real-world environment
you might wish to scale that up for better availability, network throughput,
or other reasons.

If you look at the node on which the registry landed (see the section on the
router), you'll note that it is now running on an infra worker.

Lastly, notice that the `CRD` for the image registry's configuration is not
namespaced -- it is cluster scoped. There is only one internal/integrated
registry per OpenShift cluster.

## Monitoring
The Cluster Monitoring operator is responsible for deploying and managing the
state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is
installed by default during the initial cluster installation. Its operator
uses a `ConfigMap` in the `openshift-monitoring` project to set various
tunables and settings for the behavior of the monitoring stack.

The following `ConfigMap` definition will configure the monitoring
solution to be redeployed onto infrastructure nodes.

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
```

There is no `ConfigMap` created as part of the installation. Without one, the operator will assume
default settings. Verify the `ConfigMap` is not defined in your cluster:

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

You should see:

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```

The operator will, in turn, create several `ConfigMap` objects for the
various monitoring stack components, and you can see them, too:

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

You can create the new monitoring config with the following command:

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/cluster-monitoring-configmap.yaml
----

Watch the monitoring pods move from `worker` to `infra` `Nodes` with:

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

or:

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----

## Logging
OpenShift's log aggregation solution is not installed by default. There is a
dedicated lab exercise that goes through the configuration and deployment of
logging.
