## OpenShift Infrastructure Nodes
OpenShiftのサブスクリプションモデルでは、顧客は追加料金なしで様々なコアインフラストラクチャコンポーネントをを実行できます。つまり、OpenShiftのコアインフラストラクチャコンポーネントのみを実行しているノードは、クラスター環境をカバーするために必要なサブスクリプションの総数にはカウントされません。

インフラストラクチャのカテゴライズに該当するOpenShiftコンポーネントは以下が含まれます。

* kubernetesとOpenShiftのコントロールプレーンサービス（"masters"）。
* ルータ
* コンテナイメージレジストリ
* クラスタメトリクスの収集 ("monitoring")
* クラスタ集約型ロギング
* サービスブローカー

上記以外のコンテナ/ポッド/コンポーネントを実行しているノードはすべてワーカーとみなされ、サブスクリプションでカバーされている必要があります。

### MachineSet 詳細
`MachineSets` の演習では、`MachineSets` を使用して、レプリカ数を変更してクラスタをスケーリングすることを検討しました。インフラストラクチャノードの場合、特定のKubernetesラベルを持つ `Machine` を追加で作成したいと思います。そして、それらのラベルを持つノード上で特定の動作をするように様々なインフラストラクチャコンポーネントを設定することができます。

[NOTE]
====
現在、インフラストラクチャコンポーネントの制御に使用されているOperatorは、"taint" と "toleration" の使用をすべてサポートしているわけではありません。これは、インフラストラクチャのワークロードはインフラストラクチャノード上で実行されますが、他のワークロードがインフラストラクチャノード上で実行されることは特に禁止されていないことを意味します。言い換えれば、すべてのOperatorに taint/toleration が完全に実装されるまでは、ユーザワークロードとインフラストラクチャワークロードが混在する可能性があります。

taint/tolerationの使用は、これらの演習ではカバーされていません。
====

これを実現するために、`MachineSets` を追加で作成します。

`MachineSets` がどのように動作するかを理解するためには、以下を実行してください。

[source,bash,role="copypaste copypaste-warning"]
----
oc get machineset -n openshift-machine-api -o yaml cluster-5fa6-hx2ml-worker-us-east-2c
----

#### Metadata
`MachineSet`  の `metadata` には、の `MachineSet` の名前や、様々なラベルのような情報が含まれています。


```YAML
metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400
```

[NOTE]
====
`MachineAutoScaler` が定義されている `MachineSet` をダンプした場合、`MachineSet` に `annotation` が表示されるかもしれません。
====

#### Selector
`MachineSet` は `Machine` の作成方法を定義し、`Selector` はどのマシンがそのセットに関連付けられているかをOperatorに指示します。

```YAML
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

この場合、クラスタ名は `190125-3` であり、セット全体のラベルが追加されています。

### Template Metadata
`template` は、`MachineSet` の一部で、`Machine` をテンプレート化するものです。

```YAML
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

#### Template Spec
`template` は、`Machine`/`Node` をどのように作成するかを指定する必要があります。
`spec`、より具体的には、`providerSpec`には、`Machine`を正しく作成してブートストラップするための重要なAWSデータがすべて含まれていることに気づくでしょう。

この例では、結果として得られるノードが1つ以上の特定のラベルを継承していることを確認したいと思います。上の例で見たように、ラベルは `metadata` セクションにあります。
```YAML
  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...
```

デフォルトでは、インストーラが作成する `MachineSets` は、ノードに追加のラベルを適用しません。

### カスタムMachineSetの定義
既存の`MachineSet`を分析したところで、次は作成のルールを確認してみましょう。

1. `providerSpec` の中では何も変更しない
2. `machine.openshift.io/cluster-api-cluster: <clusterid>` のインスタンスを変更しない
3. `MachineSet` にユニークな `name` を指定する
4. `machine.openshift.io/cluster-api-machineset` のインスタンスが `name` と一致することを確認する
5. ノードに必要なラベルを `.spec.template.spec.metadata.labels` に追加する
6. `MachineSet` `name` の参照を変更する場合でも、`subnet` を変更しないように注意する

一見複雑に見えますが、以下のように実行してみましょう。

[source,bash,role="execute"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=3
----

次のように実行します。

[source,bash,role="execute"]
----
oc get machineset -n openshift-machine-api
----

新しいインフラセットが以下例に似た名前で表示されているはずです。

```
...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...
```

まだインスタンスが起動していてブートストラップを行っているため、セットの中には利用可能なマシンがありません。
インスタンスがいつ実行されるかは `oc get machine -n openshift-machine-api` で確認することができます。
次に `oc get node` を使って、実際のノードがいつ結合されて準備が整ったかを確認することができます。

[NOTE]
====
`Machine` が準備されて `Node` として追加されるまでには数分かかることがあります。
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2
```

どのノードが新しいノードなのか分からなくて困っている場合は、`AGE`カラムを見てみてください。
また、`ROLES` 列では、新しいノードが `worker` と `infra` の両方のロールを持っていることに気づくでしょう。

### ラベルを確認する
この例では、一番若いノードは `ip-10-0-133-134.us-east-2.compute.internal` という名前でした。


[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-133-134.us-east-2.compute.internal --show-labels
----

そして、`LABELS` の欄には、次のように書かれています。

    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

`node-role.kubernetes.io/infra` ラベルが確認できます。

### MachineSetの追加(スケール)
現実的な本番環境では、インフラストラクチャコンポーネントを保持するために、少なくとも3つの`MachineSets`が必要になります。ロギングアグリゲーションソリューションとサービスメッシュの両方がElasticSearchをデプロイするので、ElasticSearchは3つのノードに分散した3つのインスタンスを必要とします。なぜ3つの`MachineSets`が必要なのでしょうか。理論的には、異なるAZに複数の `MachineSets` を配置することで、AWSがAZを失った場合であっても完全にダウンすることを防ぐためです。

スクリプトレットで作成した `MachineSet` はすでに3つのレプリカを作成しているので、今のところ何もする必要はありません。また、自分で追加のレプリカを作成する必要もありません。

### 追加クレジット
`openshift-machine-api` プロジェクトにはいくつかの `Pods` があります。そのうちの一つは `machine-api-controllers-56bdc6874f-86jnb` のような名前です。その `Pod` のコンテナ上で `oc log` を使うと、ノードを実際に生成するためのさまざまな演算子のビットを見ることができます。

## クイック演算子の背景
Operatorはただの `Pods` です。しかし 彼らは特別な `Pods` であり、Kubernetes環境でアプリケーションをデプロイして管理する方法を理解しているソフトウェアです。Operatorのパワーは、`CustomResourceDefinitions` (`CRD`)と呼ばれるKubernetesの機能に依存しています。`CRD`はまさにその名の通りの機能です。これらはカスタムリソースを定義する方法であり、本質的にはKubernetes APIを新しいオブジェクトで拡張するものです。

Kubernetesで `Foo` オブジェクトを作成/読み込み/更新/削除できるようにしたい場合、`Foo` リソースとは何か、どのように動作するのかを定義した `CRD` を作成します。そして、`CRD`のインスタンスである `CustomResources` (`CRs`) を作成することができます。

Operator の場合、一般的なパターンとしては、Operator が `CRs` を見て設定を行い、Kubernetes 環境上で _operate_ を行い、設定で指定されたことを実行するというものです。ここでは、OpenShiftのインフラストラクチャオペレータのいくつかがどのように動作するかを見てみましょう。

## インフラストラクチャコンポーネントの移動
これで特別なノードができたので、インフラストラクチャのコンポーネントをその上に移動させることができます。

### ルータ
OpenShiftルータは `openshift-ingress-operator` という `Operator` によって管理されています。その `Pod` は `openshift-ingress-operator` プロジェクトに存在します。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルトのルータのインスタンスは `openshift-ingress` プロジェクトにあります。 `Pods` を見てみましょう。

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

以下のように確認できます。

```
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   <none>
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   <none>
```

ルータが動作している `Node` を確認します。

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-144-70.us-east-2.compute.internal
----

`worker` の役割が指定されていることが確認できます。

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

ルータオペレータのデフォルトの設定では、`worker`の役割を持つノードを見つけてルータを配置するようになっています。しかし、専用のインフラストラクチャノードを作成したので、ルータインスタンスを `infra` の役割を持つノードに配置するようにオペレータに指示します。

OpenShiftのルーターオペレータは、`ingresses.config.openshift.io`という`CustomResourceDefinitions`(`CRD`)を使用して、クラスタのデフォルトルーティングサブドメインを定義します。

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトはmasterだけでなくルータオペレータにも観測されます。以下のようなyamlになるでしょう。

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

個々のルータのデプロイは `ingresscontrollers.operator.openshift.io` CRD で管理されます。
名前空間 `openshift-ingress-operator` に作成されたデフォルトのものがあります。


[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

以下のようになります。

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
```

ルータポッドがインフラストラクチャノードにヒットするように指示する `nodeSelector` を指定するには、以下の設定を適用します。

[source,bash,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ingresscontroller.yaml
----


実行:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[NOTE]
====
ルーターの移動中にセッションがタイムアウトすることがあります。
ページを更新してセッションを取り戻してください。
端末セッションが失われることはありませんが、手動でこのページに戻る必要があるかもしれません。
====

もし十分に手際が良ければ、`Terminating` か `ContainerCreating` のいずれかのポッドを捕まえることができるかもしれません。
Terminating` ポッドはワーカーノードの1つで動作していました。
実行中の`Running` ポッドは最終的に `infra` ロールを持つノードの1つで動作しています。

## レジストリ
レジストリは、オペレータが実際のレジストリポッドをどのように展開するかを設定するために、同様の `CRD` メカニズムを使用します。
このCRDは `configs.imageregistry.operator.openshift.io` です。
このCRDに `nodeSelector` を追加するために `cluster` のCRDオブジェクトを編集します。まず、それを見てみましょう。

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

以下のように確認できます。

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...
```

次のコマンドを実行します。

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

上記コマンドによって、レジストリCRの `.spec` を修正し、`nodeSelector` を追加します。

[NOTE]
====
この時点では、画像レジストリは演算子のために別のプロジェクトを使用していません。
演算子とオペランドは両方とも `openshift-image-registry` プロジェクトの中にあります。
====

パッチコマンドを実行すると、レジストリポッドがinfraノードに移動しているのがわかるはずです。
レジストリは `openshift-image-registry` プロジェクトにあります。

以下を素早く実行してみてください。


[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

古いレジストリポッドが終了し、新しいレジストリポッドが起動しているのがわかるかもしれません。
レジストリはS3バケットによってバックアップされているので、新しいレジストリポッドのインスタンスがどのノードにあるかは問題ではありません。
これはAPI経由でオブジェクトストアと通信しているので、そこに保存されている既存のイメージはすべてアクセス可能なままです。

また、デフォルトのレプリカ数は1であることにも注意してください。
現実の環境では、可用性やネットワークのスループットなどの理由から、このレプリカ数を増やしたいと思うかもしれません。

レジストリが着地したノード(ルータのセクションを参照)を見てみると、それが現在infraワーカー上で実行されていることに気づくでしょう。

最後に、イメージレジストリの設定のための `CRD` が名前空間ではなく、クラスタスコープになっていることに注目してください。
OpenShiftクラスタごとに内部/統合レジストリは1つしかありません。

## Monitoring
Cluster Monitoring operatorは、Prometheus+Grafana+AlertManagerによるクラスタ監視スタックの展開と状態管理を担当します。これは、クラスタの初期インストール時にデフォルトでインストールされます。このオペレータは `openshift-monitoring` プロジェクトの `ConfigMap` を利用して、監視スタックの動作のために様々なチューニングや設定を行います。

以下の `ConfigMap` 定義は、インフラストラクチャノードにデプロイされる監視ソリューションを設定するものです。


```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
```

インストールの一部として作成された `ConfigMap` は存在しません。これがない場合、Operatorはデフォルトの設定を仮定します。
クラスタに `ConfigMap` が定義されていないことを確認してください。

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

以下のように出力されるはずです。

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```

Operatorは、様々なモニタリングスタックコンポーネントのためにいくつかの `ConfigMap` オブジェクトを作成します。

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

次のコマンドで新しいモニタリング設定を作成できます。

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/cluster-monitoring-configmap.yaml
----

モニタリングポッドが `worker` から `infra` `Nodes` に移動するのを見てみましょう。

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

または

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----

## Logging
OpenShiftのログ集約ソリューションはデフォルトではインストールされていません。
ロギングの設定とデプロイメントを行う専用のラボ演習があります。
