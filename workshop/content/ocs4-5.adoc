:experimental:

= ODF オプションLab

== Labの概要
このLabはオプションで、前の4つのLabが終わった方を主な対象としています。 +

=== このLabで学習する内容

* MCGを使用してObject Bucketを作成し、アプリケーションから使用する
* 構成したCephクラスターにストレージ容量を追加する
* ODFのモニタリングを行う

== Multi-Cloud Gatewayを使用する
このセクションでは、Multi-Cloud Gateway (MCG)について説明します。現在、MCGを構成する最良の方法は、CLIを使用することです。

=== MCGステータスの確認
MCGのステータスはNooBaa CLIで確認できます。`openshift-storage` namespaceを指定して、次のコマンドを実行します。

[source,role="execute"]
----
noobaa status -n openshift-storage
----
.Example output:
----
INFO[0000] CLI version: 5.9.0
INFO[0000] noobaa-image: registry.redhat.io/odf4/mcg-core-rhel8@sha256:62c322de8d9818008b846778b2e2b51dede990c9cbf33cfe9dae23d7
44ec80d2
INFO[0000] operator-image: registry.redhat.io/odf4/mcg-rhel8-operator@sha256:ca66f681fb5435837543995440661e1ef67a5dce85ae3bf91c
afe2f0b66b6e5b
INFO[0000] noobaa-db-image: registry.redhat.io/rhel8/postgresql-12@sha256:b006af9af125adce28546d0ed9d77421abe54182741f6b5e1d31c
88dd763580b
INFO[0000] Namespace: openshift-storage
INFO[0000]
INFO[0000] CRD Status:
INFO[0000] ✅ Exists: CustomResourceDefinition "noobaas.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "backingstores.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "namespacestores.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "bucketclasses.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbucketclaims.objectbucket.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbuckets.objectbucket.io"
INFO[0000]
INFO[0000] Operator Status:
INFO[0000] ✅ Exists: Namespace "openshift-storage"
INFO[0000] ✅ Exists: ServiceAccount "noobaa"
INFO[0000] ✅ Exists: ServiceAccount "noobaa-endpoint"
INFO[0000] ✅ Exists: Role "mcg-operator.v4.9.4-noobaa-6987cf795b"
INFO[0000] ✅ Exists: Role "mcg-operator.v4.9.4-noobaa-endpoint-854658d7fc"
INFO[0000] ✅ Exists: RoleBinding "mcg-operator.v4.9.4-noobaa-6987cf795b"
INFO[0000] ✅ Exists: RoleBinding "mcg-operator.v4.9.4-noobaa-endpoint-854658d7fc"
INFO[0000] ✅ Exists: ClusterRole "mcg-operator.v4.9.4-5c7bd8bb7"
INFO[0000] ✅ Exists: ClusterRoleBinding "mcg-operator.v4.9.4-5c7bd8bb7"
INFO[0000] ✅ Exists: Deployment "noobaa-operator"
INFO[0000]
INFO[0000] System Wait Ready:
INFO[0000] ✅ System Phase is "Ready".
INFO[0000]
INFO[0000]
INFO[0000] System Status:
INFO[0000] ✅ Exists: NooBaa "noobaa"
INFO[0000] ✅ Exists: StatefulSet "noobaa-core"
INFO[0000] ✅ Exists: ConfigMap "noobaa-config"
INFO[0000] ✅ Exists: Service "noobaa-mgmt"
INFO[0000] ✅ Exists: Service "s3"
INFO[0000] ✅ Exists: Secret "noobaa-db"
INFO[0000] ✅ Exists: ConfigMap "noobaa-postgres-config"
INFO[0000] ✅ Exists: ConfigMap "noobaa-postgres-initdb-sh"
INFO[0000] ✅ Exists: StatefulSet "noobaa-db-pg"
INFO[0000] ✅ Exists: Service "noobaa-db-pg"
INFO[0000] ✅ Exists: Secret "noobaa-server"
INFO[0000] ✅ Exists: Secret "noobaa-operator"
INFO[0000] ✅ Exists: Secret "noobaa-endpoints"
INFO[0000] ✅ Exists: Secret "noobaa-admin"
INFO[0000] ✅ Exists: Secret "noobaa-root-master-key"
INFO[0001] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0001] ✅ Exists: BucketClass "noobaa-default-bucket-class"
INFO[0001] ✅ Exists: Deployment "noobaa-endpoint"
INFO[0001] ✅ Exists: HorizontalPodAutoscaler "noobaa-endpoint"
INFO[0001] ✅ (Optional) Exists: BackingStore "noobaa-default-backing-store"
INFO[0001] ✅ (Optional) Exists: CredentialsRequest "noobaa-aws-cloud-creds"
INFO[0001] ⬛ (Optional) Not Found: CredentialsRequest "noobaa-azure-cloud-creds"
INFO[0001] ⬛ (Optional) Not Found: Secret "noobaa-azure-container-creds"
INFO[0001] ⬛ (Optional) Not Found: Secret "noobaa-gcp-bucket-creds"
INFO[0001] ⬛ (Optional) Not Found: CredentialsRequest "noobaa-gcp-cloud-creds"
INFO[0001] ✅ (Optional) Exists: PrometheusRule "noobaa-prometheus-rules"
INFO[0001] ✅ (Optional) Exists: ServiceMonitor "noobaa-mgmt-service-monitor"
INFO[0001] ✅ (Optional) Exists: ServiceMonitor "s3-service-monitor"
INFO[0001] ✅ (Optional) Exists: Route "noobaa-mgmt"
INFO[0001] ✅ (Optional) Exists: Route "s3"
INFO[0001] ✅ Exists: PersistentVolumeClaim "db-noobaa-db-pg-0"
INFO[0001] ✅ System Phase is "Ready"
INFO[0001] ✅ Exists:  "noobaa-admin"

#------------------#
#- Mgmt Addresses -#
#------------------#

ExternalDNS : [https://noobaa-mgmt-openshift-storage.apps.cluster-58kkd.58kkd.sandbox557.opentlc.com https://aaad6c7c4d6134f5a8
f1d4e9212795a1-994027600.us-east-2.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.171.65:32602]
InternalDNS : [https://noobaa-mgmt.openshift-storage.svc:443]
InternalIP  : [https://172.30.250.183:443]
PodPorts    : [https://10.128.4.38:8443]

#--------------------#
#- Mgmt Credentials -#
#--------------------#

email    : admin@noobaa.io
password : YQuNVOJpUAz889bowYPkWg==

#----------------#
#- S3 Addresses -#
#----------------#

ExternalDNS : [https://s3-openshift-storage.apps.cluster-58kkd.58kkd.sandbox557.opentlc.com https://aec656477ef794b4da9f449ec3c
1fafa-693580646.us-east-2.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.171.65:31108]
InternalDNS : [https://s3.openshift-storage.svc:443]
InternalIP  : [https://172.30.69.3:443]
PodPorts    : [https://10.128.4.40:6443]

#------------------#
#- S3 Credentials -#
#------------------#

AWS_ACCESS_KEY_ID     : XXXXXXXXXXXXXXXXXXX
AWS_SECRET_ACCESS_KEY : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#------------------#
#- Backing Stores -#
#------------------#

NAME                           TYPE     TARGET-BUCKET                                                 PHASE   AGE
noobaa-default-backing-store   aws-s3   nb.1647475869927.cluster-58kkd.58kkd.sandbox557.opentlc.com   Ready   1h42m9s

#--------------------#
#- Namespace Stores -#
#--------------------#

No namespace stores found.

#------------------#
#- Bucket Classes -#
#------------------#

NAME                          PLACEMENT                                                        NAMESPACE-POLICY   PHASE   AGE

noobaa-default-bucket-class   {"tiers":[{"backingStores":["noobaa-default-backing-store"]}]}   null               Ready   1h42m
9s

#-----------------#
#- Bucket Claims -#
#-----------------#

No OBCs found.
----

NooBaa CLIは最初に環境をチェックし、次に環境に関するすべての情報を出力します。MCGのステータスに加えて、MCG bucketへの接続に使用できる使用可能なS3アドレスとS3クレデンシャルが表示されます。 +
S3アドレスはOpenShiftクラスタ内で内部的にルーティングするか、外部DNSを使用するかを選択できます。 +

ところで、*OpenShift Web Console* のダッシュボード使用することでも、MCGステータスの概要を取得できます。 +
左側のメニューから、*Storage* -> *OpenShift Data Foundation* 選択します。ダッシュボードの画面で `Status` ボックスの `1 Storage System` のリンクをクリックして、`ocs-storagecluster-storagesystem` のリンクをクリックします。 +
ODFクラスターの詳細な情報が表示される画面に移りますが、`Object` タブをクリックします。
このダッシュボードではS3 endpointの接続情報を提供しませんが、S3バックエンドの使用に関するグラフとランタイム情報を提供します。

=== Object Bucket Claimの作成

*Object Bucket Claim(OBC)* を使用することで、S3互換なbucketのバックエンドを要求できます。
OBCを作成すると、アプリケーションがオブジェクトストレージサービスを使用するために必要なすべての情報を含む *ConfigMap(CM)* と *Secret* が取得できます。

OBCの作成はNooBaa CLIを利用することで簡単に行えます。

[source,role="execute"]
----
noobaa obc create test21obc -n openshift-storage
----
.出力例:
----
INFO[0000] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0000] ✅ Created: ObjectBucketClaim "test21obc"
...(省略)
----

NooBaa CLIによって作られたOBCはOpenShiftからも確認できます。

[source,role="execute"]
----
oc get obc -n openshift-storage
----
.出力例:
----
NAME        STORAGE-CLASS                 PHASE   AGE
test21obc   openshift-storage.noobaa.io   Bound   38s
----

[source,role="execute"]
----
oc get obc test21obc -o yaml -n openshift-storage
----
.出力例:
[source,yaml,linenums]
----
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  creationTimestamp: "2022-03-17T02:00:46Z"
  finalizers:
  - objectbucket.io/finalizer
  generation: 3
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  resourceVersion: "879838"
  uid: da953376-c37a-40d5-93fb-508acdb2a19e
spec:
  bucketName: test21obc-13343e64-886a-4163-98ec-82916c13fcf3
  generateBucketName: test21obc
  objectBucketName: obc-openshift-storage-test21obc
  storageClassName: openshift-storage.noobaa.io
status:
  phase: Bound
----

`openshift-storage` namespaceの中に、このOBCを使用するための *Secret* と *ConfigMap* がありますのでこれらを確認します。
*Secret* と *ConfigMap(CM)* の名前はOBCと同じです。

[source,role="execute"]
----
oc get -n openshift-storage secret test21obc -o yaml
----
.出力例:
[source,yaml]
----
apiVersion: v1
data:
  AWS_ACCESS_KEY_ID: XXXXXXXXXXXXXXXXXXXXXXXXXX
  AWS_SECRET_ACCESS_KEY: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
kind: Secret
metadata:
  creationTimestamp: "2022-03-17T02:00:46Z"
  finalizers:
  - objectbucket.io/finalizer
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  ownerReferences:
  - apiVersion: objectbucket.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: ObjectBucketClaim
    name: test21obc
    uid: da953376-c37a-40d5-93fb-508acdb2a19e
  resourceVersion: "879832"
  uid: 2e34e696-6ed8-4427-8d6d-321b35194a57
type: Opaque
----

[source,role="execute"]
----
oc get -n openshift-storage cm test21obc -o yaml
----
.出力例:
[source,yaml]
----
apiVersion: v1
data:
  BUCKET_HOST: s3.openshift-storage.svc
  BUCKET_NAME: test21obc-13343e64-886a-4163-98ec-82916c13fcf3
  BUCKET_PORT: "443"
  BUCKET_REGION: ""
  BUCKET_SUBREGION: ""
kind: ConfigMap
metadata:
  creationTimestamp: "2022-03-17T02:00:46Z"
  finalizers:
  - objectbucket.io/finalizer
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  ownerReferences:
  - apiVersion: objectbucket.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: ObjectBucketClaim
    name: test21obc
    uid: da953376-c37a-40d5-93fb-508acdb2a19e
  resourceVersion: "879833"
  uid: edbd5e8c-3a67-49fe-99bb-0f4d7228b8f4
----

以上のようにSecretはS3アクセス用の認証情報を提供し、ConfigMapはアプリケーションのS3 endpointの情報を含んでいます。

=== PodでOBCを使用する

このセクションでは、YAMLファイルを使用してOBCを作成し、サンプルアプリケーションで提供されるS3構成を使用する方法を説明します。

OBCとサンプルアプリケーションをデプロイするには、次のYAMLファイルを適用します。

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: obc-test
---
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: obc-test
  namespace: obc-test
spec:
  generateBucketName: "obc-test-noobaa"
  storageClassName: openshift-storage.noobaa.io
---
apiVersion: batch/v1
kind: Job
metadata:
  name: obc-test
  namespace: obc-test
  labels:
    app: obc-test
spec:
  template:
    metadata:
      labels:
        app: obc-test
    spec:
      restartPolicy: OnFailure
      containers:
        - image: mesosphere/aws-cli:latest
          command: ["sh"]
          args:
            - '-c'
            - 'set -x && s3cmd --no-check-certificate --signature-v2 --host $BUCKET_HOST:$BUCKET_PORT --host-bucket $BUCKET_HOS
T:$BUCKET_PORT du'
          name: obc-test
          env:
            - name: BUCKET_NAME
              valueFrom:
                configMapKeyRef:
                  name: obc-test
                  key: BUCKET_NAME
            - name: BUCKET_HOST
              valueFrom:
                configMapKeyRef:
                  name: obc-test
                  key: BUCKET_HOST
            - name: BUCKET_PORT
              valueFrom:
                configMapKeyRef:
                  name: obc-test
                  key: BUCKET_PORT
            - name: AWS_DEFAULT_REGION
              valueFrom:
                configMapKeyRef:
                  name: obc-test
                  key: BUCKET_REGION
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: obc-test
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: obc-test
                  key: AWS_SECRET_ACCESS_KEY
----

ファイルの2番目の部分( `---` の後)では、OBCと同じ名前のConfigMapとSecretを作成するOBCを作成します(`obc-test`)。
ファイルの3番目の部分では、s3cmdが事前にインストールされたコンテナをデプロイするジョブを作成します。
このジョブではS3 endpointの現在のディスク使用量を報告するs3cmdを実行し、終了します。

それではこれを試してみましょう。

.マニフェストのデプロイ:
[source,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ocslab_obc-app-example.yaml
----
.出力例:
----
namespace/obc-test created
objectbucketclaim.objectbucket.io/obc-test created
job.batch/obc-test created
----

*Pod* が作成/実行され、最終的に `STATUS` が `Completed` になることを確認します。


[source,role="execute"]
----
oc get pods -n obc-test -l app=obc-test
----
.出力例:
----
NAME             READY   STATUS      RESTARTS   AGE
obc-test-bvg8h   0/1     Completed   0          22s
----

`obc-test` *Pod* のログから、s3cmdの出力を取得します。このテストでは、何もオブジェクトがないことが確認できます。

[source,role="execute"]
----
oc logs -n obc-test $(oc get pods -n obc-test -l app=obc-test -o jsonpath='{.items[0].metadata.name}')
----
.Example output
----
+ s3cmd --no-check-certificate --signature-v2 --host s3.openshift-storage.svc:443 --host-bucket s3.openshift-storage.svc:443 du
0        0 objects s3://obc-test-noobaa-1ec979bc-c53f-42e0-b551-ffaa895c06a6/
--------
0        Total
----

上記のように、空のbucketにアクセスすることができました。
これにより、OBCからのクレデンシャル情報が機能し、コンテナ内で正しくセットアップされていることがわかります。
ほとんどのアプリケーションはネイティブに `AWS_ACCESS_KEY_ID` と `AWS_SECRET_ACCESS_KEY` の環境変数の読み取ることをサポートしていますが、
各アプリケーションのホスト名とbucket名を設定する方法を知る必要があります。このセクションの例では、s3cmdのCLIフラグを使用しました。

== Cephクラスタへのストレージの追加

既存のODFクラスタにストレージを追加することで、容量が追加されパフォーマンスが向上されます。 +
このセクションでは、現在のストレージクラスターODF worker nodeを追加する方法について説明します。
その後、ODFクラスターを拡張してこれらの新しいノードにストレージをプロビジョニングする方法に関する次のサブセクションに続きます。

=== ODF worker nodeを追加する

ノードを追加するには、1章のように *machinesets* を追加するか、既存のODF *machiesets* をスケールアップします。
このトレーニングでは、既存のODF *machineset* をスケールアップして、より多くのworker nodeを生成します。

[NOTE]
====
ODF worker nodeを追加するときは、既存のノードに十分なCPUやメモリがない場合などが挙げられます。
====

まずは現在の *machineset* を確認します。
[source,role="execute"]
----
oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'
----
.出力例:
----
NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-58kkd-nmncw-workerocs-us-east-2a   1         1         1       1           19h
cluster-58kkd-nmncw-workerocs-us-east-2b   1         1         1       1           19h
cluster-58kkd-nmncw-workerocs-us-east-2c   1         1         1       1           19h
----

次のコマンドでworkerocs machinesetをスケールアップしましょう。

[source,role="execute"]
----
oc get machinesets -n openshift-machine-api -o name | grep workerocs | xargs -n1 -t oc scale -n openshift-machine-api --replicas=2
----
.出力例:
----
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2a
machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2a scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2b
machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2b scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2c
machineset.machine.openshift.io/cluster-58kkd-nmncw-workerocs-us-east-2c scaled
----

新しいworker nodeが使用可能になるまで待ちます。全てのカラムで `2` と表示されるまで待ちましょう。

[source,role="execute"]
----
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
----
kbd:[Ctrl+C] を押すと終了できます。

利用可能になったら、次のようにラベルを確認できます。

[source,role="execute"]
----
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}'
----
.出力例:
----
ip-10-0-131-209.us-east-2.compute.internal
ip-10-0-133-99.us-east-2.compute.internal
ip-10-0-155-12.us-east-2.compute.internal
ip-10-0-158-153.us-east-2.compute.internal
ip-10-0-160-200.us-east-2.compute.internal
ip-10-0-162-215.us-east-2.compute.internal
----

NOTE: 新しく追加したworker nodeにも `cluster.ocs.openshift.io/openshift-storage=` ラベルは既に付けられています。これは *machinesets* 自体にラベルの設定を行ったためで、新しく作られるnodeにも自動的にラベルが付けられます。

これで、クラスターを拡張するための新しいnodeを準備できました。
次のセクションに進んで、これらの空のnodeにストレージをプロビジョニングします。

=== ストレージ容量を追加する

このセクションでは、構成済みのODF worker nodeでストレージ容量とパフォーマンスを追加します。
前のセクションを実行した場合は、6つのODF worker nodeが存在するはずです。

ストレージを追加するには、*OpenShift Web Console* に移動し、手順にしたがってODFストレージクラスターの概要を表示します。

 - 左側のメニューから *Operators* -> *Installed Operators* をクリックする
 - `openshift-storage` Projectを選択する
 - `OpenShift Data Foundation Operator` をクリックする
 - 上部のナビゲーションバーで `Storage System` をクリックする
 - 表示される `ocs-storagecluster-storagesystem` の右端にある3つのドットをクリックして、オプションメニューを表示する
 - `Add Capacity` を選択し、新しいダイアログを開く

image::images/ocs/ODF-add-capacity.png[]


.Add Capacity dialog
image::images/ocs/ODF-add-capacity-dialog.png[Add Capacity dialog]

StorageClassは `gp2` を選びます。
`Raw Capacity` に表示される容量を拡張できます。ODFは三重でレプリカを取るため、`Raw Capacity` は希望する追加容量はの3倍の容量になります。

NOTE: *`Raw Capacity` は最初にODFクラスタを構成した時点で選択したストレージ容量で決まるため、変更することはできません。*

設定が完了したら、 *Add* をクリックして続行します。ストレージクラスターのステータスが再び `Ready` になるまで変化します。

新しいOSD Podが追加されていることが分かります。新しいOSD Podが、新規に追加したODF worker nodeの上で動いていることに注目して下さい。

[source,role="execute"]
----
oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd
----
.出力例:
----
rook-ceph-osd-0-9b4c7bbcc-z6cxb                                   Running     ip-10-0-171-65.us-east-2.compute.internal
rook-ceph-osd-1-78db6df7f4-jrrxg                                  Running     ip-10-0-206-84.us-east-2.compute.internal
rook-ceph-osd-2-84dfb6db48-jd2nl                                  Running     ip-10-0-157-232.us-east-2.compute.internal
rook-ceph-osd-3-55c498d8cf-hwksl                                  Running     ip-10-0-136-35.us-east-2.compute.internal
rook-ceph-osd-4-69bbb47b6b-kdmvq                                  Running     ip-10-0-179-98.us-east-2.compute.internal
rook-ceph-osd-5-5f878df68-rnsjx                                   Running     ip-10-0-202-126.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-0-data-04hwvs--1-966n4    Succeeded   ip-10-0-157-232.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-0-data-1gckv8--1-nskp5    Succeeded   ip-10-0-136-35.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-1-data-0vwlkv--1-hwg9b    Succeeded   ip-10-0-206-84.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-1-data-1q6v7s--1-4fsr4    Succeeded   ip-10-0-202-126.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-2-data-0s9hdp--1-w442h    Succeeded   ip-10-0-171-65.us-east-2.compute.internal
rook-ceph-osd-prepare-ocs-deviceset-gp2-2-data-1jrkxd--1-w5p4n    Succeeded   ip-10-0-179-98.us-east-2.compute.internal
----

以上でODFクラスターを拡張することができました。

=== 新しいストレージを確認する

容量を追加し、OSD podの存在を確認したら、toolboxを使用して追加したストレージ容量を確認することができます。

まずは toolbox Podに入ります。

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

次にCephクラスターのステータスを確認します。

[source,role="execute"]
----
ceph status
----
.出力例:
----
  cluster:
    id:     8b73e67b-6278-477a-a370-1da50f09db37
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 2h)
    mgr: a(active, since 2h)
    mds: 1/1 daemons up, 1 hot standby
    osd: 6 osds: 6 up (since 107s), 6 in (since 2m)   <--- <1>

  data:
    volumes: 1/1 healthy
    pools:   4 pools, 289 pgs
    objects: 150 objects, 219 MiB
    usage:   594 MiB used, 12 TiB / 12 TiB avail      <--- <2>
    pgs:     289 active+clean
----

この出力から次のことがわかります。

<1> 現在合計6つのOSDを使用しているが、それらは `in` で `up` である。
(つまり、OSDデーモンが実行されており、ストレージの領域として使用されている）
<2> 利用可能な物理容量が6TiBから12TiBに増加している。

これら以外にはCephステータスの出力は何も変わっていません。

続いて、Cephクラスターのトポロジーを確認します。

[source,role="execute"]
----
ceph osd crush tree
----
.出力例:
----
ID   CLASS  WEIGHT    TYPE NAME
 -1         12.00000  root default
 -5         12.00000      region us-east-2
-14          4.00000          zone us-east-2a
-13          2.00000              host ocs-deviceset-gp2-0-data-04hwvs
  2    ssd   2.00000                  osd.2
-17          2.00000              host ocs-deviceset-gp2-0-data-1gckv8
  3    ssd   2.00000                  osd.3
 -4          4.00000          zone us-east-2b
 -3          2.00000              host ocs-deviceset-gp2-2-data-0s9hdp
  0    ssd   2.00000                  osd.0
-21          2.00000              host ocs-deviceset-gp2-2-data-1jrkxd
  4    ssd   2.00000                  osd.4
-10          4.00000          zone us-east-2c
 -9          2.00000              host ocs-deviceset-gp2-1-data-0vwlkv
  1    ssd   2.00000                  osd.1
-19          2.00000              host ocs-deviceset-gp2-1-data-1q6v7s
  5    ssd   2.00000                  osd.5
----

<1> worker nodeが追加されたことで、それぞれの `zone` の中で `host` が拡張されている。

ODFで構成されたCephクラスターでは、それぞれのPoolごとにCRUSHルールが設定されています。どのルールでもデフォルトは `zone` でデータを複製するように設定されていて、高い冗長性を保ち、追加前のノードの負荷を緩和するために効果的な方法です。

.演習
次のコマンドで、それぞれのPoolごとのCRUSHルールを確認してみましょう。
[source,role="execute"]
----
ceph osd crush rule ls
----
[source,none,role="copypaste copypaste-warning"]
----
ceph osd crush rule dump <<rule name>>
----


kbd:[Ctrl+D] を押すか、`exit` を実行してtoolboxから出ることができます.

[source,role="execute"]
----
exit
----

== ODF環境のモニタリング

このセクションでは、モニタリングに関してODFで使用できるさまざまなツールについて説明します。

各種ツールは、*OpenShift Web Console* の左側メニューバーからアクセスできます。*Observe* メニューを展開し、次の3つのアイテムにアクセスします。

* Alerting
* Metrics
* Dashboards

=== Alerting

*Observe* -> *Alerting* をクリックしてアラートウィンドウを開きます。

以下に示すように *Alerting* のページに移動します。

.OCP Alerting Homepage
image::images/ocs/metrics-alertinghomepage.png[OCP Alerting Homepage]

状態ごとにメインウィンドウにアラートを表示することができます。そのためには、表示する状態をハイライトする必要があります。
アラートの状態は次のとおりです。

* `Firing` - 確認されているアラート
* `Silenced` - `Pending` や `Firing` 状態によって上がっていないアラート
* `Pending` - トリガーされたが確認されていないアラート
* `Not Firing` - トリガーされていないアラート

NOTE: 定義された時間を超えて `Pending` が継続したアラートは、 `Firing` 状態に移行します。例えば `CephClusterWarningState` の場合は10分です。

表示されているアラートをその状態に基づいてフィルタリングすることができます。表示する状態をクリックするだけでフィルターを切り替えることができます。 +
また、ウィンドウにある *Filter* を使用して特定のアラートまたはアラートのセットを検索することで、名前でフィルタリングすることもできます。

.OCP Alerting Name Filtering
image::images/ocs/metrics-alertingnamefilter.png[OCP Alert Name Filtering]

各アラートの右側にあるドット3つのアイコンから、コンテキストメニューにアクセスして、アラート定義を表示したり、アラートをサイレントにしたりできます。

.OCP Alert Contextual Menu
image::images/ocs/metrics-alertingcontextualmenu.png[OCP Alert Contextual Menu]

`View Alerting Rule` を選択すると、アラートのトリガールールの詳細にアクセスできます。
詳細には、トリガーするためにアラートが使用するPrometheusのクエリが含まれます。

.OCP Alert Detail Display
image::images/ocs/metrics-alertingviewrule.png[OCP Alert Detailed Display]

NOTE: 必要に応じて、アラートに埋め込まれたPrometheusクエリをクリックできます。
これを行うと *Metrics* ページに移動し、アラートのを実行や更新をテストすることができます。

=== Metrics

*Observe* -> *Metris* をクリックし、*Metrics* のページに移動します。

.OCP UI Metrics Homepage
image::images/ocs/metrics-queryfield.png[OCP Monitoring Metrics Homepage]

クエリフィールドを使って、式を入力するか、名前でメトリクスを検索します。
使用可能なメトリクスにより、OCP関連情報またはODF関連情報の両方を照会できます。
クエリは、Prometheusクエリ構文とそのすべての利用可能な機能を使用することができます。


簡単なクエリの例のテストしてみましょう。クエリフィールドに `ceph_osd_op` を入力し、
kbd:[Enter] を実行します。

.Simple Ceph Query
image::images/ocs/metrics-simplecephquery.png[Ceph Simple Query]

ウィンドウが更新され、次のようなグラフが表示されます。

.Simple Ceph Graph
image::images/ocs/metrics-simplecephgraph.png[Ceph Simple Graph]

次に、より複雑なクエリの例を試してみます。
クエリフィールドに `rate(ceph_osd_op[5m])` または `irate(ceph_osd_op[5m])` を入力し、kbd:[Enter] を実行します。

.Complex Ceph Query
image::images/ocs/metrics-complexcephquery.png[Ceph Complex Query]

ウィンドウが更新され、次のようなグラフが表示されます。

.Complex Ceph Graph
image::images/ocs/metrics-complexcephgraph.png[Ceph Complex Graph]

すべてのOCPメトリクスが統合された *Metrics* ウィンドウからも利用できます。
例えば、 `process_cpu_seconds_total` のようなOCP関連のメトリクスを試してみます。

.Complex OCP Graph
image::images/ocs/metrics-complexocpgraph.png[OCP Complex Graph]

`sum(irate(process_cpu_seconds_total[5m]))` と `irate(process_cpu_seconds_total[5m])` の違いを見てみましょう。

[NOTE]
====
Prometheusクエリ言語の詳細については、
link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus
Query Documentation]を参照して下さい。
====

[appendix]
== Cephの概要

このセクションでは、ODFで使用されるストレージソリューションの理解を深めるために、Cephの基礎知識を説明します

[NOTE]
====
この付録の内容は、Cephの重要なコンポーネントとCephの動作について学習することを目的としています。
ODFではOpenShiftアプリケーションにストレージを提供するために、 *Operators* と *CustomResourceDefinitions(CRDs)* を使用した方法でCephをデプロイおよび管理します。
これにより一般的なスタンドアロンのCephと比べて、Cephの高度な機能の一部が制限されていることがあります。
====

[.lead]
*Cephの歴史*

Cephプロジェクトは以下のタイムラインでわかるように長い歴史があります。

.Ceph Project History
image::images/ocs/ceph101-timeline.png[Ceph Project Timeline]

[.lead]
Cephは、OpenStackとKubernetesのストレージバックエンドとしてかなり長い間使用されてきた、歴戦のSoftware-defined Storage(SDS)ソリューションです。

[.lead]
*アーキテクチャ*

Cephクラスターは、スケーラブルなストレージソリューションを提供すると同時に、ITインフラストラクチャ内に存在するさまざまなタイプのクライアントがデータにアクセスできるように、複数のアクセス方法を提供します。

.Ceph Architecture
image::images/ocs/ceph101-overview.png[Ceph From Above]

[.lead]
CephはResilientなアーキテクチャで、単一障害点(SPOF)がありません。

[.lead]
*RADOS*

Cephの中核は、アーキテクチャ図の最下層にあるRADOS(Reliable Autonomic Distributed Object Store)と呼ばれるオブジェクトストアです。
RADOSによってCephはストレージとしてデータを保存する機能を提供します。
(つまり、IO要求を処理し、データを保護し、組み込みメカニズムによりデータの整合性と一貫性をチェックします)
RADOSは次のdaemonで構成されます。

<1> MONs or Monitors
<2> OSDs or Object Storage Devices
<3> MGRs or Managers
<4> MDSs or Meta Data Servers

.*_MONs_*
MONはCephのクラスターマップと状態を維持し、クラスターのサイズとトポロジーに応じて3または5といった奇数台で構成されます。
MONは複数台で分散意思決定を提供することでスプリットブレインの状況を防ぎます。
またMONはDataPathになく、クライアントとの間でIO要求を処理しません。

.*_OSDs_*
OSDは、データの保護(replication または erasure coding)、OSDまたはノード障害時のデータのリバランス、
データの一貫性(既存のデータのscrubbingおよびdeep-scrubbing)を保証しながら、クライアントからのIO要求を処理しています。
通常、1つのブロックデバイスごとに1つのOSDが展開され、Cephのスケーラブルな性質により、数千のOSDをクラスターに含めることができます。

.*_MGRs_*
MGRはMONと緊密に統合されており、クラスター内の統計を収集します。
さらに、Cephの機能拡張を目的としたpluggableなPythonインターフェイスを介して、拡張可能なフレームワークを提供します。
Managerフレームワークを中心に開発されたモジュールの現在のリストは次のとおりです。

* Balancer module
* Placement Group auto-scaler module
* Dashboard module
* RESTful module
* Prometheus module
* Zabbix module
* Rook module

.*_MDSs_*
MDSはディレクトリ階層やファイルのメタデータ(ownership, timestamp、modeなど)など、POSIX準拠の共有ファイルシステムのメタデータを管理します。
すべてのメタデータはRADOSで保存され、クライアントでメタデータを管理することはありません。
MDSは、CephFSによる共有ファイルシステムが構成されている場合にのみデプロイされます。

Cephクラスターの基盤の全体像はさまざまな種類のdaemonまたはコンテナによって構成されています。

.RADOS as it stands
image::images/ocs/ceph101-rados.png[RADOS Overview]

円はMONを表し、「M」はMGRを表し、バーのある四角はOSDを表します。
上の図では、クラスターは3つのMON、2つのMGR、23のOSDで動作しています。

[.lead]
*アクセス*

Cephは、すべてのアプリケーションがそのユースケースに最適なストレージを使用できるように、すべてのアクセス方法を提供するように設計されています。

.Different Storage Types Supported
image::images/ocs/ceph101-differentstoragetypes.png[Ceph Access Modes]

Cephは、

* RADOS Block Device(RBD)アクセス方式によるブロックストレージ
* Ceph Filesystem(CephFS)アクセス方式によるファイルストレージ
* ネイティブの `librados` API、またはRADOS Gateway(RADOSGWまたはRGW)によるS3/Swiftプロトコルを使用するオブジェクトストレージ

をサポートします。

[.lead]
*Librados*

Libradosを使用すると、アプリケーション開発者はのCephクラスターがネイティブに持つAPIでコーディングできるため、小さなフットプリントで大きな効率が得られます。

.Application Native Object API
image::images/ocs/ceph101-librados.png[librados]

CephのネイティブAPIは、C, C++, Python, Java, Ruby, Erlang, Go, Rustなどのさまざまなラッパーを提供します。

[.lead]
*RADOS Block Device (RBD)*

このアクセス方法は、Red Hat Enterprise LinuxまたはOpenShiftバージョン3.xまたは4.xで使用されます。
RBDは、カーネルモジュール(RHEL、OCP4) または `librbd` API(RHOSP)からアクセスできます。
OCPの世界では、RBDはRWO PVCの必要性に対処するように設計されています。

[.lead]
*_Kernel Module (kRBD)_*

kRBDドライバーは、ユーザースペースの `librbd` 方式と比較して優れたパフォーマンスを提供します。
ただし、kRBDは現在制限されており `librbd` と同じレベルの機能を提供していません。例えば、RBDミラーリングはサポートされていません。

.kRBD Diagram
image::images/ocs/ceph101-krbd.png[Kernel based RADOS Block Device]

[.lead]
*_Userspace RBD (librbd)_*

このアクセス方法は、RHEL 8.1 KernelからRed Hat OpenStackまたはOpenShiftでRBD-NBDドライバーを介して使用されます。
このモードにより、RBDミラーリングなどの既存のRBD機能をすべて活用できます。

.librbd Diagram
image::images/ocs/ceph101-librbd.png[Userspace RADOS Block Device]

[.lead]
*_共有ファイルシステム (CephFS)_*

この方法により、クライアントはPOSIX互換の共有ファイルシステムに同時にアクセスできます。
クライアントは最初にメタデータサーバーに接続して、特定のi-nodeのオブジェクトの場所を取得し、最終的にOSDと直接通信してIO要求を実行します。

.File Access (Ceph Filesystem or CephFS)
image::images/ocs/ceph101-cephfs.png[Kernel Based CephFS Client]

CephFSは通常RWXのPVCに使用されますが、RWO PVCもサポートします。

[.lead]
*_S3/Swiftオブジェクトストレージ (Ceph RADOS Gateway)_*

このアクセス方法は、Cephクラスター上でAmazon S3およびOpenStack Swift互換のオブジェクトアクセスをサポートします。
ODF MCGは、RADOSGWを活用してObject Bucket Claimを処理します。Multi-Cloud Gatewayの観点からは、RADOSGWは互換性のあるS3 endpointとしてタグ付けされます。

.Amazone S3 or OpenStack Swift (Ceph RADOS Gateway)
image::images/ocs/ceph101-rgw.png[S3 and Swift Support]

[.lead]
*CRUSH*

分散アーキテクチャであるCephクラスターは、クラスター内の複数のOSDにデータを効率的に分散するように設計されています。
そのためにCRUSH(Controlled Replication Under Scalable Hashing)と呼ばれる手法が使われます。
CRUSHでは、すべてのオブジェクトはPlacement Group(PG)と呼ばれる、1つのユニークなハッシュバケットに割り当てられます。

image::images/ocs/ceph101-crushfromobjecttoosd.png[From Object to OSD]

CRUSHはCephクラスターのトポロジー構成の中心です。
擬似ランダム配置アルゴリズムによってRADOS内のオブジェクトを分散し、CRUSHルールを使用してPGとOSDのマッピングを決定します。
本質的にPGはオブジェクト(アプリケーション層)とOSD(物理層)の間の抽象化層と言えます。
障害が発生した場合、PGは異なるOSDに再マップされ、最終的にストレージ管理者が選択したルールに一致するようにデータが再同期されます。

[.lead]
*Poolによるパーティショニング*

クラスターはPoolと呼ばれる論理的なパーティションで分割されます。各プールには次のプロパティがあります。


* Pool ID (変更不可)
* 名前
* PGの数
* PGとOSDのマッピングを決定するCRUSHルール
* データ保護のタイプ(Replication or Erasure Coding)
* データ保護のタイプに関連するパラメータ
** Rreplicated poolにおけるレプリカの数
** Erasure Coded poolにおけるチャンク数(K+M)
* クラスターの動作に影響を与えるさまざまなフラグ

[.lead]
*PoolとPG*

.Pools and PGs
image::images/ocs/ceph101-thefullpicture.png[From Object to OSD]

上の図は、クライアントIOにより保存されるオブジェクトから物理層のOSDまでのEnd-to-Endの関係を示しています。

[NOTE]
====
Poolにはサイズがなく、PGが作成されたOSDで使用可能なスペースを消費できます。また1つのPGは1つのプールのみに属します。
====

[.lead]
*データ保護*

Cephは、次の図に示す2つのタイプのデータ保護をサポートしています。

.Ceph Data Protection
image::images/ocs/ceph101-dataprotection.png[Replicated Pools vs Erasure Coded Pools]

Replicated poolは、オブジェクトを複製するため容量効率が低い(物理3バイトに対して実効は1バイト)一方で、ほとんどの場合においてErasure Coded poolよりも良好なパフォーマンスを示します。
反対にErasure Coded poolは、パフォーマンスはReplicated poolに劣る一方で、高い容量効率を示します。
Erasure Coded poolは使用するパリティの数を構成できるため、高いResiliencyと耐久性を提供できることです。
Erasure Coded poolでは次のようなK+Mの比率をサポートします。

* 4+2 (実効容量:物理容量 = 2:3)
* 8+3 (実効容量:物理容量 = 8:11)
* 8+4 (実効容量:物理容量 = 2:3)

[.lead]
*データの分散*

Cephアーキテクチャを最大限に活用するために、libradosを除くすべてのアクセス方法で、規定のサイズのオブジェクに分割して保存されます。
例えば1GBのRBDはデフォルトで4MBサイズのオブジェクトに分割されてRADOSに保存されます。CephFSやRADOSGWも同様です。

.Data Distribution
image::images/ocs/ceph101-rbdlayout.png[RADOS Block Device Layout]

[NOTE]
====
デフォルトでは、各アクセス方法は4MBのオブジェクトサイズを使用します。
上の図はRWO PVCをサポートする32MB RBDがCephクラスター全体にどのように分散して保存されるかを示しています。
====
