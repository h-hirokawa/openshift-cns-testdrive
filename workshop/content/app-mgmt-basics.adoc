= アプリケーション管理の基礎
:experimental:

== 演習の概要
このモジュールでは、`oc` ツールを使用してサンプルアプリケーションをデプロイし、OpenShift Container Platform上での中心概念、基本オブジェクト、アプリケーション管理の基本について学習します。

---

=== OpenShiftの中心概念
OpenShiftの管理者として、アプリケーションに関連するいくつかのコアとなるビルディングブロックを理解することは重要です。これらのビルディングブロックを理解することで、プラットフォーム上でのアプリケーション管理の全体像をよりよく理解することができます。

=== Projects
*Project* は一種の「バケツ」です。これは、ゆべてのユーザのリソースが存在するメタ構造です。 +
管理の観点からは、各Projectはテナントのように考えることができます。そのためProjectにはアクセスできる複数のユーザがいるかもしれませんし、ユーザは複数のProjectにアクセスできるかもしれません。 +
技術的に言えば、リソースはユーザが所有しているのではなく、Projectが所有しています。したがって、ユーザを削除しても作成されたリソースには影響しません。

この演習ではまず、いくつかのリソースを保持するProjectを作成します。

[source,bash,role="execute"]
----
oc new-project app-management
----

=== サンプルアプリケーションのデプロイ
`new-app` コマンドは、OpenShiftでアプリケーションを実行するように指示するシンプルな方法です。
ユーザはこのコマンドを使ってOpenShiftで既存のイメージを起動させたり、ソースコードをビルドしてデプロイしたり、テンプレートをインスタンス化したりするのが一般的です。 +
このコマンドに入力すべき事項のうち1つを与えるだけで、OpenShiftは何をすべきかを判断します。

次のようにQuay上に存在する特定のイメージを起動します。

[source,bash,role="execute"]
----
oc new-app quay.io/thoraxe/mapit
----

出力は以下のようになります。

----
--> Found Docker image 7ce7ade (20 months old) from quay.io for "quay.io/thoraxe/mapit"

    * An image stream tag will be created as "mapit:latest" that will track this image
    * This image will be deployed in deployment config "mapit"
    * Ports 8080/tcp, 8778/tcp, 9779/tcp will be load balanced by service "mapit"
      * Other containers can access this service through the hostname "mapit"

--> Creating resources ...
    imagestream.image.openshift.io "mapit" created
    deploymentconfig.apps.openshift.io "mapit" created
    service "mapit" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mapit'
    Run 'oc status' to view your app.
----

このコマンドの出力でOpenShiftがいくつかのリソースを自動的に作成したことがわかります。作成されたリソースを少し時間をかけて調べてみましょう。

`new-app` の機能の詳細については、 `oc new-app -h` を実行してそのヘルプメッセージを見てください。

=== Pods

.OpenShift Pods
image::images/openshift_pod.png[]

*Pod* とは、ホスト上に一緒にデプロイされた1つまたは複数のコンテナのことです。PodはOpenShiftで定義、デプロイ、管理できるコンピュートリソースの最小の単位です。 +
各PodはSDN上で独自の内部IPアドレスを割り当てられ、ポート範囲全体を所有します。また、Pod内のコンテナはローカルストレージ領域とネットワークリソースを共有できます。

PodはOpenShiftでは *静的な* オブジェクトとして扱われ、実行中にPodの定義を変更することはできません。

Podの一覧は次のように取得できます。

[source,bash,role="execute"]
----
oc get pods
----

以下のように出力されます。

----
NAME             READY   STATUS      RESTARTS   AGE
mapit-1-5qmgw    1/1     Running     0          24s
mapit-1-deploy   0/1     Completed   0          32s
----

NOTE: Pod名はデプロイプロセスの一部として動的に生成されますが、これについては後ほど説明します。

NOTE: `-deploy` Podは後述する `DeploymentConfig` に関連しています。

`oc describe` コマンドを使うと、Podの詳細を知ることができます。上記のPod名の場合、

[source,bash,role="copypaste copypaste-warning"]
----
oc describe pod mapit-1-5qmgw
----

と実行すると、以下のような出力が表示されます。

----
Name:               mapit-1-5qmgw
Namespace:          app-management
Priority:           0
PriorityClassName:  <none>
Node:               ip-10-0-139-135.ec2.internal/10.0.139.135
Start Time:         Mon, 08 Apr 2019 19:38:55 +0000
Labels:             app=mapit
                    deployment=mapit-1
                    deploymentconfig=mapit
Annotations:        k8s.v1.cni.cncf.io/networks-status:
                      [{
                          "name": "openshift-sdn",
                          "interface": "eth0",
                          "ips": [
                              "10.128.4.13"
                          ],
                          "default": true,
                          "dns": {}
                      }]
                    openshift.io/deployment-config.latest-version: 1
                    openshift.io/deployment-config.name: mapit
                    openshift.io/deployment.name: mapit-1
                    openshift.io/generated-by: OpenShiftNewApp
                    openshift.io/scc: restricted
Status:             Running
IP:                 10.128.4.13
Controlled By:      ReplicationController/mapit-1
Containers:
  mapit:
    Container ID:   cri-o://77b0d785c73faf96d911b4fb7732f4f2f75666f9377e488b59bfee3fd6a5ede6
    Image:          quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Image ID:       quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Ports:          9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Mon, 08 Apr 2019 19:39:13 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nfwnb (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-nfwnb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-nfwnb
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                                   Message
  ----    ------     ----   ----                                   -------
  Normal  Scheduled  2m16s  default-scheduler                      Successfully assigned app-management/mapit-1-5qmgw to ip-10-0-139-135.ec2.internal
  Normal  Pulling    2m7s   kubelet, ip-10-0-139-135.ec2.internal  Pulling image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
  Normal  Pulled     118s   kubelet, ip-10-0-139-135.ec2.internal  Successfully pulled image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
  Normal  Created    118s   kubelet, ip-10-0-139-135.ec2.internal  Created container mapit
  Normal  Started    118s   kubelet, ip-10-0-139-135.ec2.internal  Started container mapit
----

これは、実行しているPodの詳細な説明です。Podがどのノードで動いているか、Podの内部IPアドレス、各種ラベル、その他何が起こっているかについての情報を見ることができます。

### Services
.OpenShift Service
image::images/openshift_service.png[]

*Services* はOpenShift内部でPodのようなグループを見つけるのに便利な抽象化レイヤーを提供します。 +
*Service* はまた、それらのPodとOpenShift環境内からPodにアクセスする必要のある、他の何かとの間の内部プロキシ/ロードバランサーとしても機能します。 +
例えば、負荷を処理するためにより多くの `mapit` インスタンスが必要な場合、より多くのPodを立ち上げることができますが、OpenShiftは自動的にそれらのPodを *Service* へのエンドポイントとしてマップします。これによってアプリケーションへのリクエストはこれまでと変わらず処理され、*Service* がリクエストを処理するために良いことした以外は、何も変わったことに気づかないでしょう。

OpenShiftにイメージを実行するよう依頼することで、`new-app` コマンドが自動的に *Service* を作成しました。 +
ここで覚えていただきたいことは、*Service* はOpenShift内部のためのものであるということです。「外の世界」から利用することはできません。これについてはあとで学習します。

*Service* が一連のPodにマップされる方法は、*Labels* と *Selectors* を介して行われます。 +
*Services* には固定IPアドレスが割り当てられ、多くのポートやプロトコルをマッピングすることができます。

手作業で作成するためのYAML形式など、
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[Services]
については公式ドキュメントに多くの情報があります。

それではProject内の *Service* のリストを見てみましょう。

[source,bash,role="execute"]
----
oc get services
----

下記のように表示されます。

----
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
mapit     ClusterIP   172.30.48.204   <none>        8080/TCP,8778/TCP,9779/TCP   3m
----

NOTE: *Service* のIPアドレスは作成時に動的に割り当てられますが、これは変わることはありません。*Service* のIPアドレスは *Service* が削除されるまで予約されます。

*Pod* と同じように、*Service* も `describe` することができます。OpenShiftではほとんどのオブジェクトを `describe` することができます。

[source,bash,role="execute"]
----
oc describe service mapit
----

以下のように表示されます。

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
Annotations:       openshift.io/generated-by: OpenShiftNewApp
Selector:          app=mapit,deploymentconfig=mapit
Type:              ClusterIP
IP:                172.30.1.208
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.128.4.13:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.128.4.13:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.128.4.13:9779
Session Affinity:  None
Events:            <none>
----

すべてのオブジェクトに関する情報(それらの定義、オブジェクトの状態など)は、etcdデータストアに格納されます。 +
etcdはデータをKeyとValueのペアとして格納し、このデータはすべてシリアライズ可能なデータオブジェクト（JSON、YAML）として表すことができます。

*Service* のYAML出力を見てみましょう。

[source,bash,role="execute"]
----
oc get service mapit -o yaml
----

以下のように表示されます。

----
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-04-08T19:38:45Z
  labels:
    app: mapit
  name: mapit
  namespace: app-management
  resourceVersion: "189058"
  selfLink: /api/v1/namespaces/app-management/services/mapit
  uid: ec6ab96f-5a35-11e9-97f0-0a1014b36356
spec:
  clusterIP: 172.30.1.208
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: 8778-tcp
    port: 8778
    protocol: TCP
    targetPort: 8778
  - name: 9779-tcp
    port: 9779
    protocol: TCP
    targetPort: 9779
  selector:
    app: mapit
    deploymentconfig: mapit
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

ここで `selector` スタンザに注目し、これを覚えておきましょう。

同様に *Pod* のYAMLも調べてみて、OpenShiftがコンポーネント同士を繋げている様子を理解するのも面白いことです。 +
`mapit` Podの名前を探して、以下を実行します。

[source,bash,role="copypaste copypaste-warning"]
----
oc get pod mapit-1-5qmgw -o yaml
----

`metadata` セクションの下に、以下のように表示されているはずです。

----
  labels:
    app: mapit
    deployment: mapit-1
    deploymentconfig: mapit
  name: mapit-1-5qmgw
----

* *Service* には `app:mapit` と `deploymentconfig: mapit` を参照する `selector` スタンザがあります。
* *Pod* には複数の *Label* があります。
** `deploymentconfig: mapit`
** `app: mapit`
** `deployment: mapit-1`

*Labels* は単なるkey/valueのペアです。この *Project* 内で *Selector* に一致する *Label* を持つ *Pod* はすべて、 *Service* 関連付けられます。 +
もう一度 `oc describe` の出力を見てみると、*Service* のエンドポイントが1つあることが分かります。これはつまり、既存の `mapit` *Pod* であることがわかります。

`new-app` のデフォルトの動作は、リクエストされたアイテムのインスタンスを1つだけ作成することです。これを修正/調整する方法を見ていきますが、その前にいくつかの概念を学んでおきましょう。

### Deployment Configuration と Replication Controllers

*Service* が *Pod* のルーティングとロードバランシングを提供するのに対し、 *ReplicationControllers (RC)* は、必要な数の *Pod*(=レプリカ) を確実に存在させるために使用されます。 +
例えば、アプリケーションを常に3つの *Pod* にスケールさせておきたい場合は、*ReplicationController* が必要になります。*ReplicationController* があると、何らかの理由で終了した *Pod* は自動的に再作成されます。*ReplicationController* はOpenShiftが「自己修復」する方法を提供します。

*DeploymentConfiguration (DC)* はOpenShift内の何かをどのようにデプロイするかを定義します。以下は https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/deployments.html[deployments documentation^] の抜粋です。

----
Building on replication controllers, OpenShift adds expanded support for the
software development and deployment lifecycle with the concept of deployments.
In the simplest case, a deployment just creates a new replication controller and
lets it start up pods. However, OpenShift deployments also provide the ability
to transition from an existing deployment of an image to a new one and also
define hooks to be run before or after creating the replication controller.
----

ほとんどの場合、*Pod* , *Service* , *ReplicationController* , *DeploymentConfiguration* のリソースを一緒に使用することになります。そして、ほとんどの場合、OpenShiftがすべてのリソースを作成してくれます。

NOTE: *DC* や *Service* 存在しない *Pod* や *RC* が求められるエッジケースもありますが、これらはこの演習では説明しない高度なトピックです。

### Deploymentに関連するオブジェクトの探索

*ReplicatonController* と *DeploymentConfig* が何かが分かったので、それらがどのように動作し、どのように関連しているかを探ってみましょう。 +
OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *DeploymentConfig (DC)* を見てみましょう。

[source,bash,role="execute"]
----
oc get dc
----

以下のように表示されます。

----
NAME      REVISION   DESIRED   CURRENT   TRIGGERED BY
mapit     1          1         1         config,image(mapit:latest)
----

より詳しく知るために、*ReplicationController (RC)* について調べることができます。

OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *RC* を見てみましょう。

[source,bash,role="execute"]
----
oc get rc
----

以下のように表示されます。

----
NAME      DESIRED   CURRENT   READY     AGE
mapit-1   1         1         1         4h
----

これより、1つの *Pod* がデプロイされることが希望され (Desired)、実際にデプロイされた *Pod* が1つあることがわかります (Current)。希望する *Pod* の数を変更することで、OpenShiftに *Pod* の数を増やしたいか減らしたいかを伝えることができます。

### アプリケーションのスケーリング

`mapit` アプリケーションを2つのインスタンスまでスケールしてみましょう。これは scale コマンドで行うことができます。

[source,bash,role="execute"]
----
oc scale --replicas=2 dc/mapit
----

以下のコマンドでレプリカの数が変更されたことを確認できます。

[source,bash,role="execute"]
----
oc get rc
----

以下のように表示されます。

----
NAME         DESIRED   CURRENT   READY     AGE
mapit-1      2         2         1         4h
----

これで 2つのレプリカができたことがわかります。`oc get pods` コマンドでPodの数を確認してみましょう。

[source,bash,role="execute"]
----
oc get pods
----

以下のように表示されます。

----
NAME            READY     STATUS    RESTARTS   AGE
mapit-1-6lczv   1/1       Running   0          4h
mapit-1-rq6t6   1/1       Running   0          1m
----

そして最後に、*Service* が2つのエンドポイントを正しく反映しているかを検証してみましょう。

[source,bash,role="execute"]
----
oc describe svc mapit
----

以下のように表示されます。

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=mapit,deploymentconfig=mapit
Type:              ClusterIP
IP:                172.30.48.204
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.129.0.2:8080,10.130.0.3:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.129.0.2:8778,10.130.0.3:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.129.0.2:9779,10.130.0.3:9779
Session Affinity:  None
Events:            <none>

----

*Service* のエンドポイントを見る別の方法としては、次のようなものがあります。

[source,bash,role="execute"]
----
oc get endpoints mapit
----

すると、以下のように表示されます。

----
NAME      ENDPOINTS                                                     AGE
mapit     10.128.2.3:9779,10.129.0.3:9779,10.128.2.3:8080 + 3 more...   4h
----

各 *Pod* はOpenShift環境内で一意のIPアドレスを受信するため、IPアドレスは異なる可能性があります。エンドポイントのリストは、Serviceの背後にあるPodの数を確認する簡単な方法です。

全体的に見ると、アプリケーション(*Service* 内の *Pod*)をスケーリングすることはこのように簡単なことがわかります。 +
OpenShiftは既存のイメージの新しいインスタンスを起動しているだけなので、特にそのイメージがすでにノードにキャッシュされている場合は、アプリケーションのスケーリングは非常に早く行われることがあります。

最後に注意すべきことは、この *Service* には実際にいくつかのポートが定義されているということです。 +
先ほど、1つの *Pod* が1つのIPアドレスを取得し、そのIPアドレス上のポート空間全体を制御すると述べました。*Pod* 内で実行されている何かが複数のポートをリッスンすることがありますが(単一のコンテナが複数のポートを使用しているケース、個別のコンテナが個別のポートを使用しているケース、それらが混在しているケース、など)、*Service* は実際にはポートを異なる場所にプロキシ/マッピングすることができます。

例えば、*Service* は(レガシーな理由で)80番ポートをリッスンすることができますが、*Pod* は8080や8888などの他のポートをリッスンしている可能性があります。

この `mapit` の場合、私たちが実行したイメージは `Dockerfile` にいくつかの `EXPOSE` 文を持っていたので、OpenShiftは自動的に *Service* 上にポートを作成し、それらを *Pod* にマッピングしました。

### アプリケーションの 「セルフヒーリング」

OpenShiftの *RC* は、希望する数の *Pod* が実際に動いているかどうかを常に監視しています。そのため、何か正しくないことがあればOpenShiftが「修正」することも期待できます。

現在2つの *Pod* が稼働しているので、1つを「誤って」killしてしまった場合にどうなるか見てみましょう。`oc get pods` コマンドをもう一度実行して、*Pod* 名を選択し、次のようにしてみます。

[source,bash,role="copypaste copypaste-warning"]
----
oc delete pod mapit-1-6lczv && oc get pods
----

すると、以下のように表示されます。

----
pod "mapit-1-lhqgq" deleted
NAME            READY     STATUS              RESTARTS   AGE
mapit-1-7dw5t   1/1       Running             0          3m
mapit-1-rgnht   0/1       ContainerCreating   0          2s
----

何か気づきましたか? そうです、もう新しいコンテナが作成されています。

また、`ContainerCreating` の *Pod* の名前が変わっています。これは、OpenShiftが現在の状態(*Pod* が1つ)が希望の状態(*Pod* が2つ)と一致していないことを即座に検出し、別の *Pod* をスケジューリングして修正したためです。

### Routes
.OpenShift Route
image::images/openshift_route.png[]

*Service* はOpenShift内で内部の抽象化と負荷分散を提供しますが、OpenShift**外**のクライアント(ユーザ、システム、デバイスなど)がアプリケーションにアクセスする必要がある場合もあります。 +
外部クライアントがOpenShift内で実行されているアプリケーションにアクセスする方法は、OpenShiftのルーティングレイヤーを介して行われます。そしてその背後にあるオブジェクトが *Route* です。

デフォルトのOpenShiftルーター(HAProxy)は、着信リクエストのHTTPヘッダを使用して、どこにプロキシするかを決定します。 +
*Service*(ひいては *Pod*)に外部からアクセスできるようにしたい場合は、*Route* を作成する必要があります。オプションで *Route* に対してTLSなどのセキュリティを定義することができます。

Routerの設定を覚えていますか？おそらく覚えていないと思います。それは、OpenShiftのインストール中にRouter用のOperatorがデプロイされ、OperatorがRouterを作成したからです。 +
Routerは `openshift-ingress` *Project* にあり、以下のコマンドでその情報を見ることができます。

[source,bash,role="execute"]
----
oc describe deployment router-default -n openshift-ingress
----

NOTE: *Deployment* がKubernetesネイティブなオブジェクトであるのに対し、*DeploymentConfig* はOpenShift固有のオブジェクトであり、新しいデプロイの実行を促す
 `trigger` など、いくつかの追加機能を持っています。


RouterのOperatorについては、後続の演習で詳しく説明します。

### Route の作成
*Route* の作成は非常に簡単なプロセスです。コマンドラインから *Service* を `exporse` するだけです。 +
先ほどの *Service* の名前は `mapit` となっています。*Service* 名があれば、*Route* の作成はコマンド1つで簡単にできます。

[source,bash,role="execute"]
----
oc expose service mapit
----

このように表示されます。

----
route.route.openshift.io/mapit exposed
----

次のコマンドで *Route* が作成されたことを確認します。

[source,bash,role="execute"]
----
oc get route
----

以下のように表示されます。

----
NAME    HOST/PORT                                                           PATH   SERVICES   PORT       TERMINATION   WILDCARD
mapit   mapit-app-management.{{ ROUTE_SUBDOMAIN }}              mapit      8080-tcp
----

`HOST/PORT` 列を見ると、見慣れたFQDNが表示されています。OpenShiftはデフォルトで、定型的なホスト名で *Service* を `expose` します。

`{SERVICENAME}-{PROJECTNAME}.{ROUTINGSUBDOMAIN}`

後段のRouter Operatorラボでは、この設定とその他の設定オプションを探ります。

Routerの構成では、Routerがリッスンするドメインを指定しますが、まず最初にRouterにこれらのドメインに対するリクエストを取得する必要があります +
Routerが存在するホストに `+*.apps...+` を指すワイルドカードDNSエントリがあります。OpenShiftは *Service* 名、*Project* 名、そしてルーティングサブドメインを連結してこのFQDN/URLを作成します。

このURLにはブラウザや `curl` などのツールを使ってアクセスできます。インターネット上のどこからでもアクセスできるようにしてください。

*Route* は *Service* に関連付けられており、Routerは自動的に *Pod* に直接接続をプロキシします。Router自体は *Pod* として動作し、は「本当の」インターネットとSDNの橋渡しをします。

これまでに行ったことを見返してみると、3つのコマンドでアプリケーションをデプロイし、スケールし、外部の世界からアクセスできるようにしました。

----
oc new-app quay.io/thoraxe/mapit
oc scale --replicas=2 dc/mapit
oc expose service mapit
----

### スケールダウン
続ける前に、アプリケーションを1つのインスタンスにスケールダウンしてください。

[source,bash,role="execute"]
----
oc scale --replicas=1 dc/mapit
----

### アプリケーションのProbe
OpenShiftでは *Pod* およびコンテナ内でコマンドを実行してアプリケーションの死活をチェックすることも可能です。しかしそのコマンドは、コンテナイメージ内に既にインストールされている任意の言語を使用した複雑なスクリプトであるかもしれません。 +
OpenShiftでは、アプリケーションインスタンスの活性度(liveness)や準備状態(readiness)をチェックするための初歩的な機能が提供されています。

定義できるアプリケーションProbeには2種類あります。

*Liveness Probe*

Liveness Probeは、設定されているコンテナが実行されているかどうかをチェックします。Liveness Probeが失敗した場合、コンテナはkillされ再起動ポリシーが適用されます

*Readiness Probe*

Readiness Probeは、コンテナがリクエストをサービスする準備ができているかどうかを判断します。Readiness Probeが失敗した場合、エンドポイントのコントローラは、コンテナのIPアドレスをマッチするはずのすべての *Service* のエンドポイントから削除します。Readiness Probeは、コンテナが実行中であっても、トラフィックを受信すべきではないことをエンドポイントのコントローラに知らせるために使用することができます。

アプリケーションのProbeに関する詳細は、ドキュメントの
https://docs.openshift.com/container-platform/4.5/applications/application-health.html[Monitoring application health] セクションを参照してください。

### アプリケーションへのProbeの追加
`oc set` コマンドは、いくつかの異なる機能を実行するために使用することができますが、そのうちの1つにProbeの作成/編集があります。 +
`mapit` アプリケーションはエンドポイントを公開していますので、それが生きていて応答する準備ができているかどうかを確認することができます。
次のように `curl` を使ってテストすることが可能です。

[source,bash,role="execute"]
----
curl mapit-app-management.{{ ROUTE_SUBDOMAIN }}/health
----

いくつかのJSONが得られます。

[source,json]
----
{"status":"UP","diskSpace":{"status":"UP","total":10724835328,"free":10257825792,"threshold":10485760}}
----

以下のコマンドを使用して、OpenShiftにこのエンドポイントが生きているかどうかを調べるように依頼することができます。

[source,bash,role="execute"]
----
oc set probe dc/mapit --liveness --get-url=http://:8080/health --initial-delay-seconds=30
----

`oc describe` の出力からこのProbeが定義されていることがわかります。

[source,bash,role="execute"]
----
oc describe dc mapit
----

以下のようなセクションが表示されます。

----
...
  Containers:
   mapit:
    Image:		quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Ports:		9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:		0/TCP, 0/TCP, 0/TCP
    Liveness:		http-get http://:8080/health delay=30s timeout=1s period=10s #success=1 #failure=3
    Environment:	<none>
    Mounts:		<none>
  Volumes:		<none>
...
----

Readyiness Probeも同様にできます。

[source,bash,role="execute"]
----
oc set probe dc/mapit --readiness --get-url=http://:8080/health --initial-delay-seconds=30
----

### DeploymentConfigs と ReplicationControllers のテスト

次を実行してみて下さい。

[source,bash,role="execute"]
----
oc get pods
----

以下のように表示されます。

----
NAME             READY   STATUS      RESTARTS   AGE
mapit-1-deploy   0/1     Completed   0          18h
mapit-2-deploy   0/1     Completed   0          112s
mapit-3-deploy   0/1     Completed   0          75s
mapit-3-kkwxq    1/1     Running     0          66s
----

`-deploy` *Pod* が3つあることに注意して下さい。そして現在の `mapit` *Pod* には3という数字が含まれています。 +
これは、*DeploymentConfig* に加えた変更が、_configuration_ の変更としてカウントされ、それが新しいdeploymentの _trigger_ となったからです。`-deploy` *Pod* は、新しいdeploymentを引き起こす役割を持ちます。

次を実行して下さい。

[source,bash,role="execute"]
----
oc get deploymentconfigs
----

以下のように表示されるはずです。

----
NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
mapit   3          1         1         config,image(mapit:latest)
----

最初のdeploymentの後に2つの設定変更を行ったため、*DeploymentConfiguration* の3番目のリビジョンになっています。

以下を実行して下さい。

[source,bash,role="execute"]
----
oc get replicationcontrollers
----

次のように表示されるはずです。

----
NAME      DESIRED   CURRENT   READY   AGE
mapit-1   0         0         0       18h
mapit-2   0         0         0       5m14s
mapit-3   1         1         1       4m37s
----

新しいdeploymentがトリガーされるたびに、新しい *ReplicationController* が作成されます。 +
*ReplicationController* は *Pod* が存在することを保証する責任を持ちます。古い *RC* のスケールは0で、最新の *RC* のスケールは1であることに注意してください。

これらの *RC* をそれぞれ `oc describe` すると、`-1` にはProbeがなく、`-2` と `-3` にはそれぞれ新しいProbeがあることがわかるでしょう。