= アプリケーション管理の基礎
:experimental:

== 演習の概要
このモジュールでは、`oc` ツールを使用してサンプルアプリケーションをデプロイし、OpenShift Container Platform上での中心概念、基本オブジェクト、アプリケーション管理の基本について学習します。

---

=== OpenShiftの中心概念
OpenShiftの管理者として、アプリケーションに関連するいくつかのコアとなるビルディングブロックを理解することは重要です。これらのビルディングブロックを理解することで、プラットフォーム上でのアプリケーション管理の全体像をよりよく理解することができます。

=== Projects
*Project* は一種の「バケツ」です。これは、ゆべてのユーザのリソースが存在するメタ構造です。 +
管理の観点からは、各Projectはテナントのように考えることができます。そのためProjectにはアクセスできる複数のユーザがいるかもしれませんし、ユーザは複数のProjectにアクセスできるかもしれません。 +
技術的に言えば、リソースはユーザが所有しているのではなく、Projectが所有しています。したがって、ユーザを削除しても作成されたリソースには影響しません。

この演習ではまず、いくつかのリソースを保持するProjectを作成します。

[source,bash,role="execute"]
----
oc new-project app-management
----

=== サンプルアプリケーションのデプロイ
`new-app` コマンドは、OpenShiftでアプリケーションを実行するように指示するシンプルな方法です。
ユーザはこのコマンドを使ってOpenShiftで既存のイメージを起動させたり、ソースコードをビルドしてデプロイしたり、テンプレートをインスタンス化したりするのが一般的です。 +
このコマンドに入力すべき事項のうち1つを与えるだけで、OpenShiftは何をすべきかを判断します。

次のようにQuay上に存在する特定のイメージを起動します。

[source,bash,role="execute"]
----
oc new-app quay.io/thoraxe/mapit
----

出力は以下のようになります。

----
--> Found container image 7ce7ade (3 years old) from quay.io for "quay.io/thoraxe/mapit"

    * An image stream tag will be created as "mapit:latest" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "mapit" created
    deployment.apps "mapit" created
    service "mapit" created
--> Success
    Application is not exposed. You can expose services to the outside world by executin
g one or more of the commands below:
     'oc expose service/mapit'
    Run 'oc status' to view your app.
----

このコマンドの出力でOpenShiftがいくつかのリソースを自動的に作成したことがわかります。作成されたリソースを少し時間をかけて調べてみましょう。

`new-app` の機能の詳細については、 `oc new-app -h` を実行してそのヘルプメッセージを見てください。

=== Pods

.OpenShift Pods
image::images/openshift_pod.png[]

*Pod* とは、ホスト上に一緒にデプロイされた1つまたは複数のコンテナのことです。PodはOpenShiftで定義、デプロイ、管理できるコンピュートリソースの最小の単位です。 +
各PodはSDN上で独自の内部IPアドレスを割り当てられ、ポート範囲全体を所有します。また、Pod内のコンテナはローカルストレージ領域とネットワークリソースを共有できます。

PodはOpenShiftでは *静的な* オブジェクトとして扱われ、実行中にPodの定義を変更することはできません。

Podの一覧は次のように取得できます。

[source,bash,role="execute"]
----
oc get pods
----

以下のように出力されます。

----
NAME                     READY   STATUS    RESTARTS   AGE
mapit-764c5bf8b8-l49z7   1/1     Running   0          2m53s
----

NOTE: Pod名はデプロイプロセスの一部として動的に生成されますが、これについては後ほど説明します。

<<<<<<< HEAD
NOTE: `-deploy` Podは後述する `DeploymentConfig` に関連しています。
=======
>>>>>>> upstream/ocp4-dev

`oc describe` コマンドを使うと、Podの詳細を知ることができます。上記のPod名の場合、

[source,bash,role="execute"]
----
oc describe pod -l deployment=mapit
----

<<<<<<< HEAD
と実行すると、以下のような出力が表示されます。
=======
NOTE: The `-l deployment=mapit` selects the pod that is related to the
`Deployment` which will be discussed later.

And you will see output similar to the following:
>>>>>>> upstream/ocp4-dev

----
Name:         mapit-764c5bf8b8-l49z7
Namespace:    app-management
Priority:     0
Node:         ip-10-0-128-29.us-west-2.compute.internal/10.0.128.29
Start Time:   Tue, 10 Nov 2020 21:01:09 +0000
Labels:       deployment=mapit
              pod-template-hash=764c5bf8b8
Annotations:  k8s.v1.cni.cncf.io/network-status:
                [{
                    "name": "",
                    "interface": "eth0",
                    "ips": [
                        "10.129.2.99"
                    ],
                    "default": true,
                    "dns": {}
                }]
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "",
                    "interface": "eth0",
                    "ips": [
                        "10.129.2.99"
                    ],
                    "default": true,
                    "dns": {}
                }]
              openshift.io/generated-by: OpenShiftNewApp
              openshift.io/scc: restricted
Status:       Running
IP:           10.129.2.99
IPs:
  IP:           10.129.2.99
Controlled By:  ReplicaSet/mapit-764c5bf8b8
Containers:
  mapit:
    Container ID:   cri-o://fb708e659c19c6aaf8211bf7e3029f8adc8cf14959bcaefa5c7e6df17d37
feaf
    Image:          quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba0
9fd34b8a0dee0c4497102590d
    Image ID:       quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba0
9fd34b8a0dee0c4497102590d
    Ports:          9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Tue, 10 Nov 2020 21:01:29 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-v7fpq (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-v7fpq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-v7fpq
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason          Age    From               Message
  ----    ------          ----   ----               -------
  Normal  Scheduled       6m50s  default-scheduler  Successfully assigned app-management
/mapit-764c5bf8b8-l49z7 to ip-10-0-128-29.us-west-2.compute.internal
  Normal  AddedInterface  6m48s  multus             Add eth0 [10.129.2.99/23]
  Normal  Pulling         6m48s  kubelet            Pulling image "quay.io/thoraxe/mapit
@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
  Normal  Pulled          6m31s  kubelet            Successfully pulled image "quay.io/t
horaxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d" in
 16.762028989s
  Normal  Created         6m31s  kubelet            Created container mapit
  Normal  Started         6m31s  kubelet            Started container mapit
----

これは、実行しているPodの詳細な説明です。Podがどのノードで動いているか、Podの内部IPアドレス、各種ラベル、その他何が起こっているかについての情報を見ることができます。

### Services
.OpenShift Service
image::images/openshift_service.png[]

*Services* はOpenShift内部でPodのようなグループを見つけるのに便利な抽象化レイヤーを提供します。 +
*Service* はまた、それらのPodとOpenShift環境内からPodにアクセスする必要のある、他の何かとの間の内部プロキシ/ロードバランサーとしても機能します。 +
例えば、負荷を処理するためにより多くの `mapit` インスタンスが必要な場合、より多くのPodを立ち上げることができますが、OpenShiftは自動的にそれらのPodを *Service* へのエンドポイントとしてマップします。これによってアプリケーションへのリクエストはこれまでと変わらず処理され、*Service* がリクエストを処理するために良いことした以外は、何も変わったことに気づかないでしょう。

OpenShiftにイメージを実行するよう依頼することで、`new-app` コマンドが自動的に *Service* を作成しました。 +
ここで覚えていただきたいことは、*Service* はOpenShift内部のためのものであるということです。「外の世界」から利用することはできません。これについてはあとで学習します。

*Service* が一連のPodにマップされる方法は、*Labels* と *Selectors* を介して行われます。 +
*Services* には固定IPアドレスが割り当てられ、多くのポートやプロトコルをマッピングすることができます。

<<<<<<< HEAD
手作業で作成するためのYAML形式など、
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[Services]
については公式ドキュメントに多くの情報があります。
=======
There is a lot more information about
https://docs.openshift.com/container-platform/4.6/architecture/understanding-development.html#understanding-kubernetes-pods[Services],
including the YAML format to make one by hand, in the official documentation.
>>>>>>> upstream/ocp4-dev

それではProject内の *Service* のリストを見てみましょう。

[source,bash,role="execute"]
----
oc get services
----

下記のように表示されます。

----
NAME    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
mapit   ClusterIP   172.30.167.160   <none>        8080/TCP,8778/TCP,9779/TCP   26
----

NOTE: *Service* のIPアドレスは作成時に動的に割り当てられますが、これは変わることはありません。*Service* のIPアドレスは *Service* が削除されるまで予約されます。

*Pod* と同じように、*Service* も `describe` することができます。OpenShiftではほとんどのオブジェクトを `describe` することができます。

[source,bash,role="execute"]
----
oc describe service mapit
----

以下のように表示されます。

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
                   app.kubernetes.io/component=mapit
                   app.kubernetes.io/instance=mapit
Annotations:       openshift.io/generated-by: OpenShiftNewApp
Selector:          deployment=mapit
Type:              ClusterIP
IP:                172.30.167.160
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.129.2.99:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.129.2.99:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.129.2.99:9779
Session Affinity:  None
Events:            <none>
----

すべてのオブジェクトに関する情報(それらの定義、オブジェクトの状態など)は、etcdデータストアに格納されます。 +
etcdはデータをKeyとValueのペアとして格納し、このデータはすべてシリアライズ可能なデータオブジェクト（JSON、YAML）として表すことができます。

*Service* のYAML出力を見てみましょう。

[source,bash,role="execute"]
----
oc get service mapit -o yaml
----

以下のように表示されます。

----
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: "2020-11-10T21:01:09Z"
  labels:
    app: mapit
    app.kubernetes.io/component: mapit
    app.kubernetes.io/instance: mapit
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:openshift.io/generated-by: {}
        f:labels:
          .: {}
          f:app: {}
          f:app.kubernetes.io/component: {}
          f:app.kubernetes.io/instance: {}
      f:spec:
        f:ports:
          .: {}
          k:{"port":8080,"protocol":"TCP"}:
            .: {}
            f:name: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
          k:{"port":8778,"protocol":"TCP"}:
            .: {}
            f:name: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
          k:{"port":9779,"protocol":"TCP"}:
            .: {}
            f:name: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
        f:selector:
          .: {}
          f:deployment: {}
        f:sessionAffinity: {}
        f:type: {}
    manager: oc
    operation: Update
    time: "2020-11-10T21:01:09Z"
  name: mapit
  namespace: app-management
  resourceVersion: "106194"
  selfLink: /api/v1/namespaces/app-management/services/mapit
  uid: 921c2e2c-a53e-4f83-8e76-9df962069314
spec:
  clusterIP: 172.30.167.160
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: 8778-tcp
    port: 8778
    protocol: TCP
    targetPort: 8778
  - name: 9779-tcp
    port: 9779
    protocol: TCP
    targetPort: 9779
  selector:
    deployment: mapit
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

ここで `selector` スタンザに注目し、これを覚えておきましょう。

同様に *Pod* のYAMLも調べてみて、OpenShiftがコンポーネント同士を繋げている様子を理解するのも面白いことです。 +
`mapit` Podの名前を探して、以下を実行します。

[source,bash,role="execute"]
----
oc get pod -l deployment=mapit -o jsonpath='{.items[*].metadata.labels}' | jq -r
----

<<<<<<< HEAD
`metadata` セクションの下に、以下のように表示されているはずです。
=======
NOTE: The `-o jsonpath` selects a specific field. In this case we are asking
for the `labels` section in the manifest.

The output should look something like this:
>>>>>>> upstream/ocp4-dev

----
{
  "deployment": "mapit",
  "pod-template-hash": "764c5bf8b8"
}
----

<<<<<<< HEAD
* *Service* には `app:mapit` と `deploymentconfig: mapit` を参照する `selector` スタンザがあります。
* *Pod* には複数の *Label* があります。
** `deploymentconfig: mapit`
** `app: mapit`
** `deployment: mapit-1`
=======
* The *Service* has a `selector` stanza that refers to `deployment: mapit`.
* The *Pod* has multiple *Labels*:
** `deployment: mapit`
** `pod-template-hash: 764c5bf8b8`
>>>>>>> upstream/ocp4-dev

*Labels* は単なるkey/valueのペアです。この *Project* 内で *Selector* に一致する *Label* を持つ *Pod* はすべて、 *Service* 関連付けられます。 +
もう一度 `oc describe` の出力を見てみると、*Service* のエンドポイントが1つあることが分かります。これはつまり、既存の `mapit` *Pod* であることがわかります。

`new-app` のデフォルトの動作は、リクエストされたアイテムのインスタンスを1つだけ作成することです。これを修正/調整する方法を見ていきますが、その前にいくつかの概念を学んでおきましょう。

<<<<<<< HEAD
### Deployment Configuration と Replication Controllers

*Service* が *Pod* のルーティングとロードバランシングを提供するのに対し、 *ReplicationControllers (RC)* は、必要な数の *Pod*(=レプリカ) を確実に存在させるために使用されます。 +
例えば、アプリケーションを常に3つの *Pod* にスケールさせておきたい場合は、*ReplicationController* が必要になります。*ReplicationController* があると、何らかの理由で終了した *Pod* は自動的に再作成されます。*ReplicationController* はOpenShiftが「自己修復」する方法を提供します。

*DeploymentConfiguration (DC)* はOpenShift内の何かをどのようにデプロイするかを定義します。以下は https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/deployments.html[deployments documentation^] の抜粋です。
=======
### Background: Deployment Configurations and Replica Sets

While *Services* provide routing and load balancing for *Pods*, which may go in
and out of existence, *ReplicaSets* (RS) are used to specify and then
ensure the desired number of *Pods* (replicas) are in existence. For example, if
you always want an application to be scaled to 3 *Pods* (instances), a
*ReplicaSet* is needed. Without an RS, any *Pods* that are killed or
somehow die/exit are not automatically restarted. *ReplicaSets* are
how OpenShift "self heals".

A *Deployments* (deploy) defines how something in OpenShift should be
deployed. From the https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-kube-deployments_what-deployments-are[deployments documentation]:
>>>>>>> upstream/ocp4-dev

----
Deployments describe the desired state of a particular component of an
application as a Pod template. Deployments create ReplicaSets, which
orchestrate Pod lifecycles.
----

<<<<<<< HEAD
ほとんどの場合、*Pod* , *Service* , *ReplicationController* , *DeploymentConfiguration* のリソースを一緒に使用することになります。そして、ほとんどの場合、OpenShiftがすべてのリソースを作成してくれます。

NOTE: *DC* や *Service* 存在しない *Pod* や *RC* が求められるエッジケースもありますが、これらはこの演習では説明しない高度なトピックです。

### Deploymentに関連するオブジェクトの探索

*ReplicatonController* と *DeploymentConfig* が何かが分かったので、それらがどのように動作し、どのように関連しているかを探ってみましょう。 +
OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *DeploymentConfig (DC)* を見てみましょう。
=======
In almost all cases, you will end up using the *Pod*, *Service*,
*ReplicaSet* and *Deployment* resources together. And, in
almost all of those cases, OpenShift will create all of them for you.

There are some edge cases where you might want some *Pods* and an *RS* without a *Deployments*
or a *Service*, and others, but these are advanced topics not covered in these
exercises.

NOTE: Earlier versions of OpenShift used something called a *DeploymentConfig*.
While still a valid deployment mechanism, moving forward the *Deployment* will
be what will be created with `oc new-app`. See the
https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-comparing-deploymentconfigs_what-deployments-are[official
documentation] for more details.


### Exploring Deployment-related Objects

Now that we know the background of what a *ReplicaSet* and
*Deployment* are, we can explore how they work and are related. Take a
look at the *Deployment* (deploy) that was created for you when you told
OpenShift to stand up the `mapit` image:
>>>>>>> upstream/ocp4-dev

[source,bash,role="execute"]
----
oc get deploy
----

以下のように表示されます。

----
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mapit   1/1     1            1           76m
----

<<<<<<< HEAD
より詳しく知るために、*ReplicationController (RC)* について調べることができます。

OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *RC* を見てみましょう。
=======
To get more details, we can look into the *ReplicaSet* (*RS*).

Take a look at the *ReplicaSet* (RS) that was created for you when
you told OpenShift to stand up the `mapit` image:
>>>>>>> upstream/ocp4-dev

[source,bash,role="execute"]
----
oc get rs
----

以下のように表示されます。

----
NAME               DESIRED   CURRENT   READY   AGE
mapit-764c5bf8b8   1         1         1       77m
----

これより、1つの *Pod* がデプロイされることが希望され (Desired)、実際にデプロイされた *Pod* が1つあることがわかります (Current)。希望する *Pod* の数を変更することで、OpenShiftに *Pod* の数を増やしたいか減らしたいかを伝えることができます。

### アプリケーションのスケーリング

`mapit` アプリケーションを2つのインスタンスまでスケールしてみましょう。これは scale コマンドで行うことができます。

[source,bash,role="execute"]
----
oc scale --replicas=2 deploy/mapit
----

以下のコマンドでレプリカの数が変更されたことを確認できます。

[source,bash,role="execute"]
----
oc get rs
----

以下のように表示されます。

----
NAME               DESIRED   CURRENT   READY   AGE
mapit-764c5bf8b8   2         2         2       79m
mapit-8695cb9c67   0         0         0       79m
----

<<<<<<< HEAD
これで 2つのレプリカができたことがわかります。`oc get pods` コマンドでPodの数を確認してみましょう。
=======
NOTE: The "older" version was kept. This is to we can "rollback" to a previous
version of the application.

You can see that we now have 2 replicas. Let's verify the number of pods with
the `oc get pods` command:
>>>>>>> upstream/ocp4-dev

[source,bash,role="execute"]
----
oc get pods
----

以下のように表示されます。

----
NAME                     READY   STATUS    RESTARTS   AGE
mapit-764c5bf8b8-b4vpn   1/1     Running   0          112s
mapit-764c5bf8b8-l49z7   1/1     Running   0          81m
----

そして最後に、*Service* が2つのエンドポイントを正しく反映しているかを検証してみましょう。

[source,bash,role="execute"]
----
oc describe svc mapit
----

以下のように表示されます。

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
                   app.kubernetes.io/component=mapit
                   app.kubernetes.io/instance=mapit
Annotations:       openshift.io/generated-by: OpenShiftNewApp
Selector:          deployment=mapit
Type:              ClusterIP
IP:                172.30.167.160
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.128.2.19:8080,10.129.2.99:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.128.2.19:8778,10.129.2.99:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.128.2.19:9779,10.129.2.99:9779
Session Affinity:  None
Events:            <none>
----

*Service* のエンドポイントを見る別の方法としては、次のようなものがあります。

[source,bash,role="execute"]
----
oc get endpoints mapit
----

すると、以下のように表示されます。

----
NAME    ENDPOINTS                                                        AGE
mapit   10.128.2.19:8080,10.129.2.99:8080,10.128.2.19:9779 + 3 more...   81m
----

各 *Pod* はOpenShift環境内で一意のIPアドレスを受信するため、IPアドレスは異なる可能性があります。エンドポイントのリストは、Serviceの背後にあるPodの数を確認する簡単な方法です。

全体的に見ると、アプリケーション(*Service* 内の *Pod*)をスケーリングすることはこのように簡単なことがわかります。 +
OpenShiftは既存のイメージの新しいインスタンスを起動しているだけなので、特にそのイメージがすでにノードにキャッシュされている場合は、アプリケーションのスケーリングは非常に早く行われることがあります。

最後に注意すべきことは、この *Service* には実際にいくつかのポートが定義されているということです。 +
先ほど、1つの *Pod* が1つのIPアドレスを取得し、そのIPアドレス上のポート空間全体を制御すると述べました。*Pod* 内で実行されている何かが複数のポートをリッスンすることがありますが(単一のコンテナが複数のポートを使用しているケース、個別のコンテナが個別のポートを使用しているケース、それらが混在しているケース、など)、*Service* は実際にはポートを異なる場所にプロキシ/マッピングすることができます。

例えば、*Service* は(レガシーな理由で)80番ポートをリッスンすることができますが、*Pod* は8080や8888などの他のポートをリッスンしている可能性があります。

この `mapit` の場合、私たちが実行したイメージは `Dockerfile` にいくつかの `EXPOSE` 文を持っていたので、OpenShiftは自動的に *Service* 上にポートを作成し、それらを *Pod* にマッピングしました。

### アプリケーションの 「セルフヒーリング」

<<<<<<< HEAD
OpenShiftの *RC* は、希望する数の *Pod* が実際に動いているかどうかを常に監視しています。そのため、何か正しくないことがあればOpenShiftが「修正」することも期待できます。

現在2つの *Pod* が稼働しているので、1つを「誤って」killしてしまった場合にどうなるか見てみましょう。`oc get pods` コマンドをもう一度実行して、*Pod* 名を選択し、次のようにしてみます。
=======
Because OpenShift's *RSs* are constantly monitoring to see that the desired number
of *Pods* are actually running, you might also expect that OpenShift will "fix" the
situation if it is ever not right. You would be correct!

Now that we have two *Pods* running right now, let's see what happens when we
delete them. Frist, run the `oc get pods` command, and make note of the *Pod*
names:
>>>>>>> upstream/ocp4-dev

[source,bash,role="execute"]
----
oc get pods
----

<<<<<<< HEAD
すると、以下のように表示されます。
=======
You will see something like the following:

----
NAME                     READY   STATUS    RESTARTS   AGE
mapit-764c5bf8b8-lxnvw   1/1     Running   0          2m28s
mapit-764c5bf8b8-rscss   1/1     Running   0          2m54s
----

Now, delete the pods that belog to the *Deployment* `mapit`:
>>>>>>> upstream/ocp4-dev

[source,bash,role="execute"]
----
oc delete pods -l deployment=mapit
----

Run the `oc get pods` command once again:

[source,bash,role="execute"]
----
oc get pods
----

<<<<<<< HEAD
何か気づきましたか? そうです、もう新しいコンテナが作成されています。

また、`ContainerCreating` の *Pod* の名前が変わっています。これは、OpenShiftが現在の状態(*Pod* が1つ)が希望の状態(*Pod* が2つ)と一致していないことを即座に検出し、別の *Pod* をスケジューリングして修正したためです。
=======
Did you notice anything? There are new containers already running!

The *Pods* has a different name. That's because OpenShift almost immediately
detected that the current state (0 *Pods*, because they were deleted) didn't
match the desired state (2 *Pods*), and it
fixed it by scheduling the *Pods*.
>>>>>>> upstream/ocp4-dev

### Routes
.OpenShift Route
image::images/openshift_route.png[]

*Service* はOpenShift内で内部の抽象化と負荷分散を提供しますが、OpenShift**外**のクライアント(ユーザ、システム、デバイスなど)がアプリケーションにアクセスする必要がある場合もあります。 +
外部クライアントがOpenShift内で実行されているアプリケーションにアクセスする方法は、OpenShiftのルーティングレイヤーを介して行われます。そしてその背後にあるオブジェクトが *Route* です。

デフォルトのOpenShiftルーター(HAProxy)は、着信リクエストのHTTPヘッダを使用して、どこにプロキシするかを決定します。 +
*Service*(ひいては *Pod*)に外部からアクセスできるようにしたい場合は、*Route* を作成する必要があります。オプションで *Route* に対してTLSなどのセキュリティを定義することができます。

Routerの設定を覚えていますか？おそらく覚えていないと思います。それは、OpenShiftのインストール中にRouter用のOperatorがデプロイされ、OperatorがRouterを作成したからです。 +
Routerは `openshift-ingress` *Project* にあり、以下のコマンドでその情報を見ることができます。

[source,bash,role="execute"]
----
oc describe deployment router-default -n openshift-ingress
----

<<<<<<< HEAD
NOTE: *Deployment* がKubernetesネイティブなオブジェクトであるのに対し、*DeploymentConfig* はOpenShift固有のオブジェクトであり、新しいデプロイの実行を促す
 `trigger` など、いくつかの追加機能を持っています。


RouterのOperatorについては、後続の演習で詳しく説明します。
=======
You will explore the Operator for the router more in a subsequent exercise.
>>>>>>> upstream/ocp4-dev

### Route の作成
*Route* の作成は非常に簡単なプロセスです。コマンドラインから *Service* を `exporse` するだけです。 +
先ほどの *Service* の名前は `mapit` となっています。*Service* 名があれば、*Route* の作成はコマンド1つで簡単にできます。

[source,bash,role="execute"]
----
oc expose service mapit
----

このように表示されます。

----
route.route.openshift.io/mapit exposed
----

次のコマンドで *Route* が作成されたことを確認します。

[source,bash,role="execute"]
----
oc get route
----

以下のように表示されます。

----
NAME    HOST/PORT                                             PATH   SERVICES   PORT       TERMINATION   WILDCARD
mapit   mapit-app-management.{{ ROUTE_SUBDOMAIN }}                   mapit      8080-tcp                 None
----

`HOST/PORT` 列を見ると、見慣れたFQDNが表示されています。OpenShiftはデフォルトで、定型的なホスト名で *Service* を `expose` します。

`{SERVICENAME}-{PROJECTNAME}.{ROUTINGSUBDOMAIN}`

後段のRouter Operatorラボでは、この設定とその他の設定オプションを探ります。

Routerの構成では、Routerがリッスンするドメインを指定しますが、まず最初にRouterにこれらのドメインに対するリクエストを取得する必要があります +
Routerが存在するホストに `+*.apps...+` を指すワイルドカードDNSエントリがあります。OpenShiftは *Service* 名、*Project* 名、そしてルーティングサブドメインを連結してこのFQDN/URLを作成します。

このURLにはブラウザや `curl` などのツールを使ってアクセスできます。インターネット上のどこからでもアクセスできるようにしてください。

*Route* は *Service* に関連付けられており、Routerは自動的に *Pod* に直接接続をプロキシします。Router自体は *Pod* として動作し、は「本当の」インターネットとSDNの橋渡しをします。

これまでに行ったことを見返してみると、3つのコマンドでアプリケーションをデプロイし、スケールし、外部の世界からアクセスできるようにしました。

----
oc new-app quay.io/thoraxe/mapit
oc scale --replicas=2 deploy/mapit
oc expose service mapit
----

### スケールダウン
続ける前に、アプリケーションを1つのインスタンスにスケールダウンしてください。

[source,bash,role="execute"]
----
oc scale --replicas=1 deploy/mapit
----

### アプリケーションのProbe
OpenShiftでは *Pod* およびコンテナ内でコマンドを実行してアプリケーションの死活をチェックすることも可能です。しかしそのコマンドは、コンテナイメージ内に既にインストールされている任意の言語を使用した複雑なスクリプトであるかもしれません。 +
OpenShiftでは、アプリケーションインスタンスの活性度(liveness)や準備状態(readiness)をチェックするための初歩的な機能が提供されています。

定義できるアプリケーションProbeには2種類あります。

*Liveness Probe*

Liveness Probeは、設定されているコンテナが実行されているかどうかをチェックします。Liveness Probeが失敗した場合、コンテナはkillされ再起動ポリシーが適用されます

*Readiness Probe*

Readiness Probeは、コンテナがリクエストをサービスする準備ができているかどうかを判断します。Readiness Probeが失敗した場合、エンドポイントのコントローラは、コンテナのIPアドレスをマッチするはずのすべての *Service* のエンドポイントから削除します。Readiness Probeは、コンテナが実行中であっても、トラフィックを受信すべきではないことをエンドポイントのコントローラに知らせるために使用することができます。

<<<<<<< HEAD
アプリケーションのProbeに関する詳細は、ドキュメントの
https://docs.openshift.com/container-platform/4.5/applications/application-health.html[Monitoring application health] セクションを参照してください。
=======
More information on probing applications is available in the
https://docs.openshift.com/container-platform/4.6/applications/application-health.html[Application
Health] section of the documentation.
>>>>>>> upstream/ocp4-dev

### アプリケーションへのProbeの追加
`oc set` コマンドは、いくつかの異なる機能を実行するために使用することができますが、そのうちの1つにProbeの作成/編集があります。 +
`mapit` アプリケーションはエンドポイントを公開していますので、それが生きていて応答する準備ができているかどうかを確認することができます。
次のように `curl` を使ってテストすることが可能です。

[source,bash,role="execute"]
----
curl mapit-app-management.{{ ROUTE_SUBDOMAIN }}/health
----

いくつかのJSONが得られます。

[source,json]
----
{"status":"UP","diskSpace":{"status":"UP","total":10724835328,"free":10257825792,"threshold":10485760}}
----

以下のコマンドを使用して、OpenShiftにこのエンドポイントが生きているかどうかを調べるように依頼することができます。

[source,bash,role="execute"]
----
oc set probe deploy/mapit --liveness --get-url=http://:8080/health --initial-delay-seconds=30
----

`oc describe` の出力からこのProbeが定義されていることがわかります。

[source,bash,role="execute"]
----
oc describe deploy mapit
----

以下のようなセクションが表示されます。

----
...
  Containers:
   mapit:
    Image:        quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda
4594ba09fd34b8a0dee0c4497102590d
    Ports:        9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:   0/TCP, 0/TCP, 0/TCP
    Liveness:     http-get http://:8080/health delay=30s timeout=1s period=10s
#success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
...
----

Readyiness Probeも同様にできます。

[source,bash,role="execute"]
----
<<<<<<< HEAD
oc set probe dc/mapit --readiness --get-url=http://:8080/health --initial-delay-seconds=30
----

### DeploymentConfigs と ReplicationControllers のテスト

次を実行してみて下さい。

[source,bash,role="execute"]
----
oc get pods
----

以下のように表示されます。

----
NAME             READY   STATUS      RESTARTS   AGE
mapit-1-deploy   0/1     Completed   0          18h
mapit-2-deploy   0/1     Completed   0          112s
mapit-3-deploy   0/1     Completed   0          75s
mapit-3-kkwxq    1/1     Running     0          66s
----

`-deploy` *Pod* が3つあることに注意して下さい。そして現在の `mapit` *Pod* には3という数字が含まれています。 +
これは、*DeploymentConfig* に加えた変更が、_configuration_ の変更としてカウントされ、それが新しいdeploymentの _trigger_ となったからです。`-deploy` *Pod* は、新しいdeploymentを引き起こす役割を持ちます。
=======
oc set probe deploy/mapit --readiness --get-url=http://:8080/health --initial-delay-seconds=30
----

### Examining Deployments and ReplicaSets

Each change to the *Deployment* is counted as a _configuration_ change, which
_triggers_ a new _deployment_. The *Deployment* in in charge of which
*ReplicaSet* to deploy. The _newest_ is always deployed.
>>>>>>> upstream/ocp4-dev

次を実行して下さい。

[source,bash,role="execute"]
----
oc get deployments
----

以下のように表示されるはずです。

----
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mapit   1/1     1            1           131m
----

<<<<<<< HEAD
最初のdeploymentの後に2つの設定変更を行ったため、*DeploymentConfiguration* の3番目のリビジョンになっています。
=======
You made two material configuration changes (plus a scale), after the initial deployment,
thus you are now on the fourth revision of the *Deployment*.
>>>>>>> upstream/ocp4-dev

以下を実行して下さい。

[source,bash,role="execute"]
----
oc get replicasets
----

次のように表示されるはずです。

----
NAME               DESIRED   CURRENT   READY   AGE
mapit-5f695ff4b8   1         1         1       4m19s
mapit-668f69cdd5   0         0         0       6m18s
mapit-764c5bf8b8   0         0         0       133m
mapit-8695cb9c67   0         0         0       133m
----

<<<<<<< HEAD
新しいdeploymentがトリガーされるたびに、新しい *ReplicationController* が作成されます。 +
*ReplicationController* は *Pod* が存在することを保証する責任を持ちます。古い *RC* のスケールは0で、最新の *RC* のスケールは1であることに注意してください。

これらの *RC* をそれぞれ `oc describe` すると、`-1` にはProbeがなく、`-2` と `-3` にはそれぞれ新しいProbeがあることがわかるでしょう。
=======
Each time a new deployment is triggered, the deployer pod creates a new
*ReplicaSet* which then is responsible for ensuring that pods
exist. Notice that the old RSs have a desired scale of zero, and the most
recent RS has a desired scale of 1.

If you `oc describe` each of these RSs you will see how earlier versions have
no probes, and the latest running RS has the new probes.
>>>>>>> upstream/ocp4-dev
