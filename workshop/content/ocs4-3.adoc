:experimental:

= Lab 2-3: ODFが提供するRWO PVを使用する

== Labの概要
このLabでは作成したODFクラスターからReadWriteOnce(RWO)のPersistent Volume(PV)を作成し、アプリケーションから使用します。 +
RWO PVは、ひとつのworker node上で稼働するアプリケーションPodにBindされ、PodがマウントすることでアプリケーションはPVにRead/Writeすることができます。 +
通常PVをどのnodeにアタッチするかを意識することはありませんが、RWO PVはひとつのworker nodeにのみアタッチされます。ReadWriteOnceの"Once"はこれを意味します。 +
複数のworker nodeにアタッチし、その上で稼働する複数のアプリケーションPodが共有してRead-Writeすることはできません。これにはRWX(ReadWriteMany)というタイプのPVが必要となります。RWX PVは次のLabで学習します。 +

=== このLabで学習する内容

* PVC(Persistent Volume Claim)を発行し、PVが作られることを確認する
* RWO(ReadWriteOnce) PVCを使ったアプリケーションをデプロイする
* 作成したPVの実体であるCeph RBDボリュームを確認する


[[labexercises]]

== 2-3-1. PVC(Persistent Volume Claim)の発行

アプリケーションがPVを利用する典型的な方法の1つに、PVC(Persistent Volume Claim)を発行するものがあります。 +
PVCは"Claim"という名の通り、OCPクラスターに対してPVの要求を行うものです。PVCを受け取ったOCPクラスターは、要求の内容に合致するPVを探してPVCに返答します。

つまり、通常はPVCが発行される前にPVが作られている事が必要となります。ただし、*Dynamic Provisioning* に対応しているストレージの場合は、PVCが発行された後にその要求通りのPVをリアルタイムに作成してPVCに返すことができます。

試しにシンプルなPVCを発行してみましょう。まず、PVを確認してみます。

[source,role="execute"]
----
oc get pv
----
.出力例
----
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM
                                       STORAGECLASS                  REASON   AGE
pvc-1e2b927e-afc0-4132-b95a-d8c761529200   50Gi       RWO            Delete           Bound    openshift-sto
rage/rook-ceph-mon-b                   gp2                                    57m
pvc-34f039fb-1d22-4332-a9ec-0dc42b7e4275   50Gi       RWO            Delete           Bound    openshift-sto
rage/rook-ceph-mon-a                   gp2                                    57m
pvc-47f8c6ba-cf0b-4d99-8d66-2f055627ce35   2Ti        RWO            Delete           Bound    openshift-sto
rage/ocs-deviceset-gp2-2-data-0s9hdp   gp2                                    48m
pvc-8988551a-8f22-4a2f-bfb8-d68b55b38758   2Ti        RWO            Delete           Bound    openshift-sto
rage/ocs-deviceset-gp2-0-data-04hwvs   gp2                                    48m
pvc-8ae00cde-f2f6-4d48-95a5-80725cacef06   50Gi       RWO            Delete           Bound    openshift-sto
rage/db-noobaa-db-pg-0                 ocs-storagecluster-ceph-rbd            46m
pvc-ada6bd42-caec-4d58-b3b7-42bc8c69635c   50Gi       RWO            Delete           Bound    openshift-sto
rage/rook-ceph-mon-c                   gp2                                    57m
pvc-f48dfb9e-f01f-40c0-b5ce-84f5d1aa7b6e   2Ti        RWO            Delete           Bound    openshift-sto
rage/ocs-deviceset-gp2-1-data-0vwlkv   gp2                                    48m
----

いくつかのPVが表示されますが、これらはODFクラスターを構成するために作られたPVです。ユーザーアプリケーション向けにはPVはまだありません。

それでは下のようにPVCを作成します。 +
ODFをバックエンドとする `ocs-storagecluster-ceph-rbd` StorageClass に対して、1GiBのRWO PVを要求する内容です。

[source,role="execute"]
----
cat << EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ocs-storagecluster-ceph-rbd
  resources:
    requests:
      storage: 1Gi
EOF
----

次のコマンドで"example"という名前のPVCが作成されていることが確認できます。

[source,role="execute"]
----
oc get pvc -n default
----
.出力例
----
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
example   Bound    pvc-f87e20a8-cbd0-4e7a-9789-86eee0ae9691   1Gi        RWO            ocs-storagecluster-ceph-rbd   5s
----

再度PVを確認してみると、先程はなかったPVが作成されていることが分かります。

[source,role="execute"]
----
oc get pv
----
.出力例
----
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                STORAGECLASS                  REASON   AGE
...(省略)
pvc-f87e20a8-cbd0-4e7a-9789-86eee0ae9691   1Gi        RWO            Delete           Bound    default/example                                      ocs-storagecluster-ceph-rbd            16s
...(省略)
----

これはODFが *Dynamic Provisioning* に対応しているため、PVCの発行後自動的にODFにボリュームを作るよう命令し、そのボリュームがPVとして登録されるからです。

== 2-3-2. RWO(ReadWriteOnce) PVCを使ったアプリケーションをデプロイする

このセクションでは、`ocs-storagecluster-ceph-rbd` StorageClassを使ってRWO(ReadWriteOnce) Presistent Volume Claimを作成し、RailsアプリケーションとPostgreSQLデータベースをデプロイします。

NOTE: Rails + PostgreSQLのデプロイを開始できるように、前のセクションをすべて完了したことを確認してください。

OpenShift rails-pgsql-persistentテンプレートに基づいたテンプレートファイルを次のリンク先に作成しています。

`https://raw.githubusercontent.com/tutsunom/ocs-training/jp/ocp4ocs4/configurable-rails-app.yaml`

このファイルには、PVCが使用するStorageClassをエンドユーザーが指定できる追加のパラメーター `STORAGE_CLASS` が含まれています。ダウンロードして確認してみて下さい。

以下のコマンドでアプリケーションのデプロイを開始します。

[source,role="execute"]
----
oc new-project my-database-app
oc new-app -f {{ HOME_PATH }}/support/ocslab_rails-app.yaml -p STORAGE_CLASS=ocs-storagecluster-ceph-rbd -p VOLUME_CAPACITY=5Gi
----

デプロイが始まったら `oc status` コマンドでデプロイの様子を監視できます。

[source,role="execute"]
----
oc status
----
.出力例
----
In project my-database-app on server https://172.30.0.1:443

svc/postgresql - 172.30.44.10:5432
  dc/postgresql deploys openshift/postgresql:10
    deployment #1 running for 5 seconds - 0/1 pods

http://rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com (svc/rails-pgsql-persistent)
  dc/rails-pgsql-persistent deploys istag/rails-pgsql-persistent:latest <-
    bc/rails-pgsql-persistent source builds https://github.com/sclorg/rails-ex.git on openshift/ruby:2.5
      build #1 pending for 6 seconds
    deployment #1 waiting on image or update

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
----

以下に示すように、2つのpodが `Running` STATUSで、4つのpodが `Completed` STATUSになるまで待ちます。
このステップには5分以上かかる場合があります。

[source,role="execute"]
----
watch oc get pods -n my-database-app
----
.出力例:
----
NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          84s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          81s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          62s
----
kbd:[Ctrl+C] を押すと終了できます。

次に、PVCを確認します。先程のテンプレートファイルの中にPVCのマニフェストが記載されているので、PVCが発行されています。PVCが作られていることを確認しましょう。

[source,role="execute"]
----
oc get pvc -n my-database-app
----

ODFでRWO PVCで作られるPVの実体は、`ocs-storagecluster-cephblockpool` プール内に作られるCeph RBD(RADOS Block Device) imageです。 +
アプリケーションがPersistent VolumeとしてCeph RBDボリュームを使用しているかどうかテストできます。

[source,role="execute"]
----
oc get route -n my-database-app
----
.出力例:
----
NAME                     HOST/PORT                                                                         PATH   SERVICES                 PORT    TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com          rails-pgsql-persistent
----

`rails-pgsql-persistent` routeをブラウザウィンドウにコピーし、末尾に `/articles` を追加したURLにアクセスします。

*Example*  `http://rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com/articles`

Webページの *New Article* をクリックし、次の `username` と `password` を入力することで記事やコメントを作成することができます。 +

[source,ini]
----
username: openshift
password: secret
----

何でもよいので、ここで1つ記事を作成してください。

作成された記事とコメントはPostgreSQLデータベースに保存されます。PostgreSQLデータベースは、アプリケーションのデプロイ中に `ocs-storagecluster-ceph-rbd` *StorageClass* を使ってプロビジョニングされたCeph RBDボリュームにテーブルスペースを保存します。 +
そのため、PostgreSQLのPodを削除してもデータが失われることはありません。試しにPostgreSQLのPodを削除してみましょう。 +
PostgreSQLのPodはDeploymentConfigによって削除されても自動的に再作成され、すでに存在するPVを自動でマウントするようになっています。

[source,role="execute"]
----
oc delete $(oc get pod -l name=postgresql -n my-database-app -o name) -n my-database-app
----

.ターミナルのプロンプトが戻ってくるまで待って下さい。
CAUTION: プロンプトが戻ってくるまで数分かかる場合があります。

PostgreSQLのPodが再作成されたら、再びRailsのWebアプリケーションにアクセスしてみて下さい。キャッシュを消しても先に書いた記事が残っていることが確認できます。

== 2-3-3. 作成したPVの実体であるCeph RBDボリュームを確認する

先程作成したPVは、`ocs-storagecluster-cephblockpool` プール内に作られるCeph RBD(RADOS Block Device)ボリュームです。ここではPVとCeph RBDボリュームとがどのように対応しているか確認してみます。

ここでtoolboxにログインして、`ocs-storagecluster-cephblockpool` をもう一度見てみましょう。

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

下記のようにアプリケーションのデプロイ前と同じCephコマンドを実行し、前のセクションの結果と比較します。
`ocs-storagecluster-cephblockpool` のオブジェクト数が増えていることに注意して下さい。 +
また、3つ目のコマンドはCeph RBDボリュームをリストする処理をしますが、3つ表示されるはずです。

[source,role="execute"]
----
ceph df
----
[source,role="execute"]
----
rados df
----
[source,role="execute"]
----
rbd -p ocs-storagecluster-cephblockpool ls | grep vol
----
kbd:[Ctrl+D] を押すか、 `exit` を実行してtoolboxから出ることができます。

[source,role="execute"]
----
exit
----

どのPVがどのCeph RBDに対応するかの同定を行ってみましょう。 +
次のコマンドを実行してPVの `Volume Handle` を確認します。

[source,role="execute"]
----
oc get pv -o 'custom-columns=NAME:.spec.claimRef.name,PVNAME:.metadata.name,STORAGECLASS:.spec.storageClassName,VOLUMEHANDLE:.spec.csi.volumeHandle'
----
.出力例:
----
NAME                              PVNAME                                     STORAGECLASS                  VOLUMEHANDLE
rook-ceph-mon-b                   pvc-1e2b927e-afc0-4132-b95a-d8c761529200   gp2                           <none>
rook-ceph-mon-a                   pvc-34f039fb-1d22-4332-a9ec-0dc42b7e4275   gp2                           <none>
ocs-deviceset-gp2-2-data-0s9hdp   pvc-47f8c6ba-cf0b-4d99-8d66-2f055627ce35   gp2                           <none>
ocs-deviceset-gp2-0-data-04hwvs   pvc-8988551a-8f22-4a2f-bfb8-d68b55b38758   gp2                           <none>
db-noobaa-db-pg-0                 pvc-8ae00cde-f2f6-4d48-95a5-80725cacef06   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-00
00000000000001-bf425b04-a586-11ec-bdd5-0a580a80041e
rook-ceph-mon-c                   pvc-ada6bd42-caec-4d58-b3b7-42bc8c69635c   gp2                           <none>
postgresql                        pvc-debec901-780f-44b5-8b71-2d32d2fe32fd   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-00
00000000000001-29fbab33-a58f-11ec-bdd5-0a580a80041e
example                           pvc-e3962c93-52ba-4cb0-b55f-2a831d40dda6   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-00
00000000000001-5db07ddc-a58d-11ec-bdd5-0a580a80041e
ocs-deviceset-gp2-1-data-0vwlkv   pvc-f48dfb9e-f01f-40c0-b5ce-84f5d1aa7b6e   gp2                           <none>
----

`VOLUMEHANDLE` カラムの後半部分は、Ceph RBDの名前と一致していることがわかります。この前に `csi-vol-` をつけることで完全なRBDを取得することができます。 +

[source,role="execute"]
----
CSIVOL=$(oc get pv $(oc get pv | grep my-database-app | awk '{ print $1 }') -o jsonpath='{.spec.csi.volumeHandle}' | cut -d '-' -f 6- | awk '{print "csi-vol-"$1}')
echo $CSIVOL
----

例えば、toolboxと組み合わせてCeph RBDボリュームの詳細を確認できます。

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD rbd -p ocs-storagecluster-cephblockpool info $CSIVOL
----

.出力例:
----
rbd image 'csi-vol-29fbab33-a58f-11ec-bdd5-0a580a80041e':
        size 5 GiB in 1280 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 5f0043c253e9
        block_name_prefix: rbd_data.5f0043c253e9
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Mar 17 01:11:24 2022
        access_timestamp: Thu Mar 17 01:11:24 2022
        modify_timestamp: Thu Mar 17 01:11:24 2022
----

---
以上で、「Lab 2-3: ODFが提供するRWO PVを使用する」は完了です。 +
次は link:ocs4-4[Lab 2-4: CephFSボリュームを使ってRWX PVを使用する] に進みます。